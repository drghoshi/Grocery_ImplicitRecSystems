{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main purpose of this .ipynb file is to generate the training and validation sets required\n",
    "# for our stage 2 ranking model. Required is the file containing the top X predictions per user\n",
    "# generated from Pipeline_Main_Stage1.ipynb. Returned from this notebook are the training set,\n",
    "# validation set, and ground truth set indicated which items were actually purcahsed by the user\n",
    "# in their last order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9caeff47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03635734018919414"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.09981707889495871-0.09631531038969282)/0.09631531038969282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "febab9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from scipy import spatial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2002d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in pickle file containing top X predictions\n",
    "\n",
    "cg_exploded = pd.read_pickle(\"~/work/cg_exploded_rank250_20k_100.pkl\")\n",
    "\n",
    "# read in 0-based user_id index map\n",
    "\n",
    "file_name = \"user_study_dict_20k.pkl\"\n",
    "#file_name = 'user_study_dict_ALLUSERS.pkl'\n",
    "open_file = open(file_name, \"rb\")\n",
    "user_zerobased_map = pickle.load(open_file)\n",
    "open_file.close()\n",
    "reverse_user_map = {v: k for k, v in user_zerobased_map.items()}\n",
    "\n",
    "# convert user_id back to original 1-based index\n",
    "\n",
    "cg_exploded.user_id = cg_exploded.user_id.apply(lambda x: reverse_user_map[x])\n",
    "user_study = user_zerobased_map.keys()\n",
    "cg_exploded['candidate_generated_set'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f0725",
   "metadata": {},
   "source": [
    "#### Load Historical Order Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8fe2d994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n",
      "merge prior and orders and keep train separate ...\n"
     ]
    }
   ],
   "source": [
    "myfolder='~/work/'\n",
    "prior = pd.read_csv(myfolder + 'order_products__prior.csv', dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16, 'add_to_cart_order': np.uint8})\n",
    "\n",
    "train_orders = pd.read_csv(myfolder + 'order_products__train.csv', dtype={'order_id': np.uint32,\n",
    "           'product_id': np.uint16, 'reordered': np.int8, 'add_to_cart_order': np.uint8 })\n",
    "\n",
    "orders = pd.read_csv(myfolder + 'orders.csv', dtype={'order_hour_of_day': np.uint8,\n",
    "           'order_number': np.uint8, 'order_id': np.uint32, 'user_id': np.uint32,\n",
    "           'order_dow': np.uint8, 'days_since_prior_order': np.float16})\n",
    "\n",
    "orders.eval_set = orders.eval_set.replace({'prior': 0, 'train': 1, 'test':2}).astype(np.uint8)\n",
    "\n",
    "orders.days_since_prior_order = orders.days_since_prior_order.fillna(30).astype(np.uint8)\n",
    "\n",
    "products = pd.read_csv(myfolder + 'products.csv', dtype={'product_id': np.uint16,\n",
    "            'aisle_id': np.uint8, 'department_id': np.uint8},\n",
    "             usecols=['product_id', 'aisle_id', 'department_id'])\n",
    "\n",
    "print('done loading')\n",
    "print('merge prior and orders and keep train separate ...')\n",
    "\n",
    "orders_products_prior = orders.merge(prior, how = 'inner', on = 'order_id')\n",
    "orders_products_prior = orders_products_prior.merge(products, how='inner', on='product_id')\n",
    "#orders_products_prior.to_csv('instacart_prior_set.csv', index=False)\n",
    "\n",
    "orders_products_train = orders.merge(train_orders, how='inner', on='order_id')\n",
    "orders_products_train = orders_products_train.merge(products, how='inner', on='product_id')\n",
    "orders_products_train['purchased_label'] = 1\n",
    "#orders_products_train.to_csv('instacart_train_set.csv', index=False)\n",
    "\n",
    "train_orders = train_orders.merge(orders[['user_id','order_id']], left_on = 'order_id', right_on = 'order_id', how = 'inner')\n",
    "\n",
    "all_products = products.product_id.unique()\n",
    "all_products.sort()\n",
    "product_zerobased_map = dict(zip(all_products, range(len(all_products))))\n",
    "\n",
    "with open('np_instacart_product_vectors.pkl', 'rb') as handle:\n",
    "    np_instacart_product_vectors = pickle.load(handle)\n",
    "vector_list_instacart_products = np_instacart_product_vectors.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7ffb4",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f379540c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# returns a generator serving n records at a time\n",
    "\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "def grab_users_hard_negatives(user, products_purchased, n, tree):\n",
    "    closest_neighbors = pd.Series(products_purchased, name='product_id').apply(lambda x: grab_closest_n_neighbors(n, x, tree)[0:n])\n",
    "    df_hard_negatives = pd.concat([pd.Series([user]*len(closest_neighbors), name='user_id'), closest_neighbors, \n",
    "                                  pd.Series([0]*len(closest_neighbors), name='interaction')], axis=1)\n",
    "    return df_hard_negatives.explode('product_id')\n",
    "\n",
    "# def update_neighbor_dict(user_cohort_purchased_products, n):\n",
    "#     for x in user_cohort_purchased_products:\n",
    "#         if x in dict_all_nn:\n",
    "#             closest_neighbor_dict[x] = dict_all_nn[x]\n",
    "#         else:\n",
    "#             closest_neighbor_dict[x] = grab_closest_n_neighbors(n, x, tree)\n",
    "#     return closest_neighbor_dict\n",
    "\n",
    "def update_neighbor_dict(user_cohort_purchased_products, dict_all_nn, n):\n",
    "    for x in user_cohort_purchased_products:\n",
    "        if x not in dict_all_nn:\n",
    "            dict_all_nn[x] =  grab_closest_n_neighbors(n, x, tree)\n",
    "    return dict_all_nn\n",
    "\n",
    "tree = spatial.cKDTree(vector_list_instacart_products)\n",
    "def grab_closest_n_neighbors(n, product_id, tree):\n",
    "    study_index_vector = vector_list_instacart_products[product_id]\n",
    "    tree_search_indices = tree.query(study_index_vector, k=5, workers=-1)[1].tolist()\n",
    "    try:\n",
    "        tree_search_indices.remove(product_id)\n",
    "        return tree_search_indices[0:n]\n",
    "    except:\n",
    "        return tree_search_indices[0:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1836705",
   "metadata": {},
   "source": [
    "#### Extract all Relevant Prior Interactions (Purchases)\n",
    "- This will serve as the training set for stage 2 ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4f7fddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(filename, orders_products_prior, products, user_study, include_hard_negative, n_hard_negatives, all_positives, include_random_popularity_sampling, n_rns):\n",
    "    \n",
    "    # filtering previous purchase data for population we've selected (user_study)\n",
    "    df_interaction_prior = orders_products_prior[orders_products_prior.user_id.isin(user_study)][[\"user_id\", \"product_id\"]].reset_index(drop=True)\n",
    "    #df_interaction_prior = orders_products_prior[[\"user_id\", \"product_id\"]].reset_index(drop=True)\n",
    "\n",
    "    # specifying that our target variable is 1 for all previously purchased items\n",
    "    df_interaction_prior['interaction'] = 1\n",
    "\n",
    "    # taking into account the popularity of each item for sampling purposes later\n",
    "\n",
    "    relative_frequencies = pd.DataFrame(orders_products_prior.product_id.value_counts(normalize=True))\n",
    "    relative_frequencies.rename({'product_id': 'frequency'}, axis=1, inplace=True)\n",
    "    relative_frequencies['product_id'] = relative_frequencies.index\n",
    "    relative_frequencies.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df = products[[\"product_id\"]].copy()\n",
    "\n",
    "    list_user_df = []\n",
    "    counter = 0\n",
    "    dict_all_nn = {}\n",
    "    for user_id_group in batch(df_interaction_prior.user_id.unique(), 500):\n",
    "        counter += 500\n",
    "        print (counter)\n",
    "        # creating our negative training examples\n",
    "        df_user = pd.concat([pd.concat([df.copy() for x in range(len(user_id_group))], axis=0).reset_index(drop=True), \n",
    "                             pd.concat([pd.Series(np.ones(products.shape[0]) * user_id, name='user_id') for user_id in user_id_group], axis=0).reset_index(drop=True)], axis=1)\n",
    "        df_user[\"interaction\"] = 0\n",
    "\n",
    "        # concatenating positive (purchased items) and negative training examples\n",
    "        df_user = pd.concat([df_interaction_prior[df_interaction_prior.user_id.isin(user_id_group)], df_user], \n",
    "                                axis=0, ignore_index=True)\n",
    "\n",
    "        # removing instances where we have a positive and negative instance for the same product. The negative instance\n",
    "        # will be dropped and the positive example will remain\n",
    "        df_user['distinct_count'] = df_user.groupby(by=[\"user_id\", \"product_id\"])['interaction'].transform('nunique')\n",
    "        df_user_unique = df_user[~((df_user.interaction == 0) & (df_user.distinct_count == 2))].reset_index(drop=True)\n",
    "        df_user_unique.drop('distinct_count', axis=1, inplace=True)\n",
    "\n",
    "        # converting user_id and product_id back to their original index\n",
    "        df_user_unique.user_id = df_user_unique.user_id.astype(int)\n",
    "        df_user_unique.product_id = df_user_unique.product_id.astype(int)\n",
    "        df_user_unique.product_id = df_user_unique.product_id - 1\n",
    "        original_shape = pd.merge(df_user_unique, cg_exploded[cg_exploded.user_id.isin(user_id_group)], how='inner', on=['user_id', 'product_id']).shape[0]\n",
    "        if all_positives:\n",
    "            # want to include both products that are included in our top X predictions as well as all previously purchased items\n",
    "            df_user_unique = pd.merge(df_user_unique, cg_exploded[cg_exploded.user_id.isin(user_id_group)], how='left', on=['user_id', 'product_id'])\n",
    "            df_user_unique = df_user_unique[(df_user_unique.candidate_generated_set == True) | (test.interaction == 1)]\n",
    "            df_user_unique.user_id = df_user_unique.user_id.apply(lambda x: user_zerobased_map[x])\n",
    "            print (str(df_user_unique.shape[0] - original_shape), 'positives were added from historical purchases not in top K predictions from ALS.')\n",
    "        else:\n",
    "            # only want to include products that are included in our top X predictions from stage 1 model\n",
    "            df_user_unique = pd.merge(df_user_unique, cg_exploded[cg_exploded.user_id.isin(user_id_group)], how='inner', on=['user_id', 'product_id'])\n",
    "            df_user_unique.user_id = df_user_unique.user_id.apply(lambda x: user_zerobased_map[x])\n",
    "        df_user_unique.drop(['candidate_generated_set'], axis=1 ,inplace=True)\n",
    "        original_shape = df_user_unique.shape[0]\n",
    "        # add in random popularity sampling\n",
    "        if include_random_popularity_sampling:\n",
    "            unique_positive_count = df_user_unique[df_user_unique.interaction == 1].drop_duplicates().shape[0]\n",
    "            sample_product_ids = random.choices(list(relative_frequencies.product_id), list(relative_frequencies.frequency), \n",
    "                                          k=unique_positive_count*n_rns)\n",
    "            negative_random_samples = pd.concat([pd.concat([df_user_unique[df_user_unique.interaction == 1].drop_duplicates().user_id.reset_index(drop=True) for i in range(n_rns)], axis=0).reset_index(drop=True), \n",
    "                                                 pd.Series(sample_product_ids, name='product_id'), \n",
    "                                                 pd.Series([0] * unique_positive_count*n_rns, name='interaction')], axis=1)\n",
    "            df_user_unique = pd.concat([df_user_unique, negative_random_samples], axis=0)\n",
    "            df_user_unique.reset_index(drop=True, inplace=True)\n",
    "            df_user_unique['distinct_count'] = df_user_unique.groupby(by=[\"user_id\", \"product_id\"])['interaction'].transform('nunique')\n",
    "            df_user_unique = df_user_unique[~((df_user_unique.interaction == 0) & (df_user_unique.distinct_count == 2))].reset_index(drop=True)\n",
    "            df_user_unique.drop('distinct_count', axis=1, inplace=True)\n",
    "            print (str(df_user_unique.shape[0] - original_shape), 'negatives were added from random negative sampling.')\n",
    "        original_shape = df_user_unique.shape[0]\n",
    "        # add in hard negatives based on parameter, n\n",
    "        if include_hard_negative:\n",
    "\n",
    "            df_nn = df_user_unique.copy()\n",
    "            df_nn = df_nn.drop_duplicates()\n",
    "            df_nn = df_nn[df_nn.interaction == 1].copy()\n",
    "            user_cohort_purchased_products = df_user_unique[df_user_unique.interaction == 1].product_id.unique()\n",
    "            start = time.time()\n",
    "            dict_all_nn = update_neighbor_dict(user_cohort_purchased_products, dict_all_nn, n_hard_negatives)\n",
    "            end = time.time()\n",
    "            print (len(dict_all_nn.items()), 'items in dictionary.')\n",
    "\n",
    "            print ('completed neighbor search in', np.round((end-start), 2), 'seconds.')\n",
    "\n",
    "            df_nn['nearest_neighbors'] = df_nn.product_id.map(dict_all_nn)\n",
    "            df_nn = df_nn.explode('nearest_neighbors')\n",
    "            df_nn.drop(['product_id'], axis=1, inplace=True)\n",
    "            df_nn = df_nn.rename({'nearest_neighbors': 'product_id'}, axis=1)\n",
    "            df_nn['interaction'] = 0\n",
    "            df_user_unique = pd.concat([df_user_unique, df_nn], axis=0).reset_index(drop=True)\n",
    "            df_user_unique['distinct_count'] = df_user_unique.groupby(by=[\"user_id\", \"product_id\"])['interaction'].transform('nunique')\n",
    "            df_user_unique = df_user_unique[~((df_user_unique.interaction == 0) & (df_user_unique.distinct_count == 2))].reset_index(drop=True)\n",
    "            df_user_unique.drop('distinct_count', axis=1, inplace=True)\n",
    "            print (str(df_user_unique.shape[0] - original_shape), 'negatives were added from hard negative sampling.')\n",
    "    #         user_cohort_purchased_products = df_user_unique[df_user_unique.interaction == 1].product_id.unique()\n",
    "\n",
    "    #         start = time.time()\n",
    "    #         closest_neighbor_dict = update_neighbor_dict(user_cohort_purchased_products, n)\n",
    "    #         end = time.time()\n",
    "    #         print ('completed neighbor search in', np.round((end-start), 2), 'seconds.')\n",
    "    #         dict_all_nn.update(closest_neighbor_dict)\n",
    "\n",
    "    #         df_closest_neighbors = pd.DataFrame(closest_neighbor_dict.items(), columns=['product_id', 'nearest_neighbors']).explode('nearest_neighbors').reset_index(drop=True)\n",
    "    #         df_closest_neighbors['interaction'] = 0\n",
    "    #         hard_negatives = pd.merge(df_user_unique[df_user_unique.interaction == 1].drop_duplicates(), df_closest_neighbors, how='left', on='product_id')\n",
    "    #         hard_negatives.drop(['interaction_x', 'product_id'], axis=1, inplace=True)\n",
    "    #         hard_negatives = hard_negatives.rename({'interaction_y': 'interaction', 'nearest_neighbors': 'product_id'}, axis=1)\n",
    "    #         df_user_unique = pd.concat([df_user_unique, hard_negatives], axis=0).reset_index(drop=True)\n",
    "    #         df_user_unique['distinct_count'] = df_user_unique.groupby(by=[\"user_id\", \"product_id\"])['interaction'].transform('nunique')\n",
    "    #         df_user_unique = df_user_unique[~((df_user_unique.interaction == 0) & (df_user_unique.distinct_count == 2))].reset_index(drop=True)\n",
    "    #         df_user_unique.drop('distinct_count', axis=1, inplace=True)\n",
    "\n",
    "        print ('null values:', df_user_unique[df_user_unique.product_id.isnull()].shape[0])\n",
    "        print ('')\n",
    "        print ('')\n",
    "        # shuffling data for better training mechanics later\n",
    "        df_user_unique = df_user_unique.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # writing to file\n",
    "        df_user_unique.to_csv(filename, mode='a', index=False, header=(not os.path.exists(filename)))\n",
    "\n",
    "        del df_user\n",
    "        del df_user_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b58b9f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing\n",
    "\n",
    "# include_hard_negative = False\n",
    "# all_positives = True\n",
    "# n_hard_negatives = 0\n",
    "# include_random_popularity_sampling = True\n",
    "\n",
    "\n",
    "# # filtering previous purchase data for population we've selected (user_study)\n",
    "# df_interaction_prior = orders_products_prior[orders_products_prior.user_id.isin(user_study)][[\"user_id\", \"product_id\"]].reset_index(drop=True)\n",
    "# #df_interaction_prior = orders_products_prior[[\"user_id\", \"product_id\"]].reset_index(drop=True)\n",
    "\n",
    "# # specifying that our target variable is 1 for all previously purchased items\n",
    "# df_interaction_prior['interaction'] = 1\n",
    "\n",
    "# # taking into account the popularity of each item for sampling purposes later\n",
    "\n",
    "# relative_frequencies = pd.DataFrame(orders_products_prior.product_id.value_counts(normalize=True))\n",
    "# relative_frequencies.rename({'product_id': 'frequency'}, axis=1, inplace=True)\n",
    "# relative_frequencies['product_id'] = relative_frequencies.index\n",
    "# relative_frequencies.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df = products[[\"product_id\"]].copy()\n",
    "\n",
    "# list_user_df = []\n",
    "# counter = 0\n",
    "# dict_all_nn = {}\n",
    "# for user_id_group in batch(df_interaction_prior.user_id.unique(), 500):\n",
    "#     counter += 500\n",
    "#     print (counter)\n",
    "#     # creating our negative training examples\n",
    "#     df_user = pd.concat([pd.concat([df.copy() for x in range(len(user_id_group))], axis=0).reset_index(drop=True), \n",
    "#                          pd.concat([pd.Series(np.ones(products.shape[0]) * user_id, name='user_id') for user_id in user_id_group], axis=0).reset_index(drop=True)], axis=1)\n",
    "#     df_user[\"interaction\"] = 0\n",
    "\n",
    "#     # concatenating positive (purchased items) and negative training examples\n",
    "#     df_user = pd.concat([df_interaction_prior[df_interaction_prior.user_id.isin(user_id_group)], df_user], \n",
    "#                             axis=0, ignore_index=True)\n",
    "\n",
    "#     # removing instances where we have a positive and negative instance for the same product. The negative instance\n",
    "#     # will be dropped and the positive example will remain\n",
    "#     df_user['distinct_count'] = df_user.groupby(by=[\"user_id\", \"product_id\"])['interaction'].transform('nunique')\n",
    "#     df_user_unique = df_user[~((df_user.interaction == 0) & (df_user.distinct_count == 2))].reset_index(drop=True)\n",
    "#     df_user_unique.drop('distinct_count', axis=1, inplace=True)\n",
    "\n",
    "#     # converting user_id and product_id back to their original index\n",
    "#     df_user_unique.user_id = df_user_unique.user_id.astype(int)\n",
    "#     df_user_unique.product_id = df_user_unique.product_id.astype(int)\n",
    "#     df_user_unique.product_id = df_user_unique.product_id - 1\n",
    "#     original_shape = df_user_unique.shape[0]\n",
    "#     if all_positives:\n",
    "#         # want to include both products that are included in our top X predictions as well as all previously purchased items\n",
    "#         df_user_unique = pd.merge(df_user_unique, cg_exploded[cg_exploded.user_id.isin(user_id_group)], how='left', on=['user_id', 'product_id'])\n",
    "#         df_user_unique = df_user_unique[(df_user_unique.candidate_generated_set == True) | (test.interaction == 1)]\n",
    "#         df_user_unique.user_id = df_user_unique.user_id.apply(lambda x: user_zerobased_map[x])\n",
    "#         print (str(df_user_unique.shape[0] - original_shape), 'positives were added from historical purchases not in top K predictions from ALS.')\n",
    "#     else:\n",
    "#         # only want to include products that are included in our top X predictions from stage 1 model\n",
    "#         df_user_unique = pd.merge(df_user_unique, cg_exploded[cg_exploded.user_id.isin(user_id_group)], how='inner', on=['user_id', 'product_id'])\n",
    "#         df_user_unique.user_id = df_user_unique.user_id.apply(lambda x: user_zerobased_map[x])\n",
    "#     df_user_unique.drop(['candidate_generated_set'], axis=1 ,inplace=True)\n",
    "#     original_shape = df_user_unique.shape[0]\n",
    "#     # add in random popularity sampling\n",
    "#     if include_random_popularity_sampling:\n",
    "#         unique_positive_count = df_user_unique[df_user_unique.interaction == 1].drop_duplicates().shape[0]\n",
    "#         sample_product_ids = random.choices(list(relative_frequencies.product_id), list(relative_frequencies.frequency), \n",
    "#                                       k=unique_positive_count*n_rns)\n",
    "#         negative_random_samples = pd.concat([pd.concat([df_user_unique[df_user_unique.interaction == 1].drop_duplicates().user_id.reset_index(drop=True) for i in range(n_rns)], axis=0).reset_index(drop=True), \n",
    "#                                              pd.Series(sample_product_ids, name='product_id'), \n",
    "#                                              pd.Series([0] * unique_positive_count*n_rns, name='interaction')], axis=1)\n",
    "#         df_user_unique = pd.concat([df_user_unique, negative_random_samples], axis=0)\n",
    "#         df_user_unique.reset_index(drop=True, inplace=True)\n",
    "#         df_user_unique['distinct_count'] = df_user_unique.groupby(by=[\"user_id\", \"product_id\"])['interaction'].transform('nunique')\n",
    "#         df_user_unique = df_user_unique[~((df_user_unique.interaction == 0) & (df_user_unique.distinct_count == 2))].reset_index(drop=True)\n",
    "#         df_user_unique.drop('distinct_count', axis=1, inplace=True)\n",
    "#         print (str(df_user_unique.shape[0] - original_shape), 'negatives were added from random negative sampling.')\n",
    "#     original_shape = df_user_unique.shape[0]\n",
    "#     # add in hard negatives based on parameter, n\n",
    "#     if include_hard_negative:\n",
    "\n",
    "#         df_nn = df_user_unique.copy()\n",
    "#         df_nn = df_nn.drop_duplicates()\n",
    "#         df_nn = df_nn[df_nn.interaction == 1].copy()\n",
    "#         user_cohort_purchased_products = df_user_unique[df_user_unique.interaction == 1].product_id.unique()\n",
    "#         start = time.time()\n",
    "#         dict_all_nn = update_neighbor_dict(user_cohort_purchased_products, dict_all_nn, n_hard_negatives)\n",
    "#         end = time.time()\n",
    "#         print (len(dict_all_nn.items()), 'items in dictionary.')\n",
    "\n",
    "#         print ('completed neighbor search in', np.round((end-start), 2), 'seconds.')\n",
    "\n",
    "#         df_nn['nearest_neighbors'] = df_nn.product_id.map(dict_all_nn)\n",
    "#         df_nn = df_nn.explode('nearest_neighbors')\n",
    "#         df_nn.drop(['product_id'], axis=1, inplace=True)\n",
    "#         df_nn = df_nn.rename({'nearest_neighbors': 'product_id'}, axis=1)\n",
    "#         df_nn['interaction'] = 0\n",
    "#         df_user_unique = pd.concat([df_user_unique, df_nn], axis=0).reset_index(drop=True)\n",
    "#         df_user_unique['distinct_count'] = df_user_unique.groupby(by=[\"user_id\", \"product_id\"])['interaction'].transform('nunique')\n",
    "#         df_user_unique = df_user_unique[~((df_user_unique.interaction == 0) & (df_user_unique.distinct_count == 2))].reset_index(drop=True)\n",
    "#         df_user_unique.drop('distinct_count', axis=1, inplace=True)\n",
    "#         print (str(df_user_unique.shape[0] - original_shape), 'negatives were added from hard negative sampling.')\n",
    "# #         user_cohort_purchased_products = df_user_unique[df_user_unique.interaction == 1].product_id.unique()\n",
    "\n",
    "# #         start = time.time()\n",
    "# #         closest_neighbor_dict = update_neighbor_dict(user_cohort_purchased_products, n)\n",
    "# #         end = time.time()\n",
    "# #         print ('completed neighbor search in', np.round((end-start), 2), 'seconds.')\n",
    "# #         dict_all_nn.update(closest_neighbor_dict)\n",
    "\n",
    "# #         df_closest_neighbors = pd.DataFrame(closest_neighbor_dict.items(), columns=['product_id', 'nearest_neighbors']).explode('nearest_neighbors').reset_index(drop=True)\n",
    "# #         df_closest_neighbors['interaction'] = 0\n",
    "# #         hard_negatives = pd.merge(df_user_unique[df_user_unique.interaction == 1].drop_duplicates(), df_closest_neighbors, how='left', on='product_id')\n",
    "# #         hard_negatives.drop(['interaction_x', 'product_id'], axis=1, inplace=True)\n",
    "# #         hard_negatives = hard_negatives.rename({'interaction_y': 'interaction', 'nearest_neighbors': 'product_id'}, axis=1)\n",
    "# #         df_user_unique = pd.concat([df_user_unique, hard_negatives], axis=0).reset_index(drop=True)\n",
    "# #         df_user_unique['distinct_count'] = df_user_unique.groupby(by=[\"user_id\", \"product_id\"])['interaction'].transform('nunique')\n",
    "# #         df_user_unique = df_user_unique[~((df_user_unique.interaction == 0) & (df_user_unique.distinct_count == 2))].reset_index(drop=True)\n",
    "# #         df_user_unique.drop('distinct_count', axis=1, inplace=True)\n",
    "\n",
    "#     print ('null values:', df_user_unique[df_user_unique.product_id.isnull()].shape[0])\n",
    "#     print ('')\n",
    "#     print ('')\n",
    "#     # shuffling data for better training mechanics later\n",
    "#     df_user_unique = df_user_unique.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#     # writing to file\n",
    "#     df_user_unique.to_csv(filename, mode='a', index=False, header=(not os.path.exists(filename)))\n",
    "\n",
    "#     del df_user\n",
    "#     del df_user_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "62900e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([df_user_unique[df_user_unique.interaction == 1].drop_duplicates().user_id.reset_index(drop=True) for i in range(2)], axis=0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# negative_random_samples\n",
    "\n",
    "# relative_frequencies = pd.DataFrame(orders_products_prior.product_id.value_counts(normalize=True))\n",
    "# relative_frequencies.rename({'product_id': 'frequency'}, axis=1, inplace=True)\n",
    "# relative_frequencies['product_id'] = relative_frequencies.index\n",
    "# relative_frequencies.reset_index(drop=True, inplace=True)\n",
    "# sample_product_ids = choices(list(relative_frequencies.product_id), list(relative_frequencies.frequency), \n",
    "#                                       k=orders_products_prior.shape[0])\n",
    "\n",
    "# pd.merge(relative_frequencies, pd.read_csv('~/work/products.csv'), how='left', on='product_id')\n",
    "\n",
    "# import random\n",
    "\n",
    "# df_user_unique\n",
    "\n",
    "# unique_positive_count = df_user_unique[df_user_unique.interaction == 1].drop_duplicates().shape[0]\n",
    "\n",
    "# sample_product_ids = random.choices(list(relative_frequencies.product_id), list(relative_frequencies.frequency), \n",
    "#                                       k=unique_positive_count)\n",
    "\n",
    "# negative_random_samples = pd.concat([df_user_unique[df_user_unique.interaction == 1].drop_duplicates().user_id.reset_index(drop=True), pd.Series(sample_product_ids, name='product_id'), pd.Series([0] * unique_positive_count, name='interaction')], axis=1)\n",
    "\n",
    "# negative_random_samples\n",
    "\n",
    "# df_user_unique = pd.concat([df_user_unique, negative_random_samples], axis=0)\n",
    "# df_user_unique.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 24918838 - df_user_unique.shape[0]\n",
    "\n",
    "# pd.merge(pd.Series(sample_product_ids, name='product_id'), pd.read_csv('~/work/products.csv'), how='left', on='product_id')\n",
    "\n",
    "# relative_frequencies\n",
    "\n",
    "# sample_product_ids = choices(list(relative_frequencies.product_id), list(relative_frequencies.frequency), \n",
    "#                                       k=orders_products_prior.shape[0])\n",
    "\n",
    "# df_user_unique.head()\n",
    "\n",
    "# cg_exploded['candidate_generated_set'] = True\n",
    "\n",
    "# pd.merge(df_user_unique, cg_exploded[cg_exploded.user_id.isin(user_id_group)], how='inner', on=['user_id', 'product_id'])\n",
    "\n",
    "# test = pd.merge(df_user_unique, cg_exploded[cg_exploded.user_id.isin(user_id_group)], how='left', on=['user_id', 'product_id'])\n",
    "\n",
    "# test = test[(test.candidate_generated_set == True) | (test.interaction == 1)]\n",
    "\n",
    "# test[test.candidate_generated_set.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4d9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aa0cae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_data(filename, orders_products_train, products, user_study):\n",
    "# filtering previous purchase data for population we've selected (user_study)\n",
    "    df_interaction_train = orders_products_train[orders_products_train.user_id.isin(user_study)][[\"user_id\", \"product_id\"]].reset_index(drop=True)\n",
    "    df_interaction_train['interaction'] = 1\n",
    "\n",
    "    \n",
    "    df_user_zerobased_map = pd.DataFrame.from_dict(user_zerobased_map, orient='index', columns=['zerobased'])\n",
    "    df_user_zerobased_map['user_id'] = df_user_zerobased_map.index\n",
    "    df_user_zerobased_map.reset_index(drop=True, inplace=True)\n",
    "    df = products[[\"product_id\"]].copy()\n",
    "    list_user_df = []\n",
    "    counter = 0\n",
    "    for user_id_group in batch(df_interaction_train.user_id.unique(), 500):\n",
    "        counter += 500\n",
    "\n",
    "        print (counter)\n",
    "\n",
    "        # creating our negative training examples\n",
    "        df_user = pd.concat([pd.concat([df.copy() for x in range(len(user_id_group))], axis=0).reset_index(drop=True), \n",
    "                             pd.concat([pd.Series(np.ones(products.shape[0]) * user_id, name='user_id') for user_id in user_id_group], axis=0).reset_index(drop=True)], axis=1)\n",
    "        df_user[\"interaction\"] = 0\n",
    "\n",
    "        # concatenating positive (purchased items) and negative training examples\n",
    "        df_user = pd.concat([df_interaction_train[df_interaction_train.user_id.isin(user_id_group)], df_user], \n",
    "                                axis=0, ignore_index=True)\n",
    "\n",
    "        # removing instances where we have a positive and negative instance for the same product. The negative instance\n",
    "        # will be dropped and the positive example wil remain\n",
    "        df_user['distinct_count'] = df_user.groupby(by=[\"user_id\", \"product_id\"])['interaction'].transform('nunique')\n",
    "        df_user_unique = df_user[~((df_user.interaction == 0) & (df_user.distinct_count == 2))].reset_index(drop=True)\n",
    "        df_user_unique.drop('distinct_count', axis=1, inplace=True)\n",
    "        df_user_unique.user_id = df_user_unique.user_id.astype(int)\n",
    "        df_user_unique.product_id = df_user_unique.product_id - 1\n",
    "\n",
    "        # only want to include products that are included in our top X predictions from stage 1 model\n",
    "        df_user_unique = pd.merge(df_user_unique, cg_exploded[cg_exploded.user_id.isin(user_id_group)], how='inner', on=['user_id', 'product_id'])\n",
    "\n",
    "        # converting user_id and product_id back to their original index\n",
    "        df_user_unique = pd.merge(df_user_unique, df_user_zerobased_map, how='left', on='user_id')\n",
    "        df_user_unique.drop(['user_id'], axis=1, inplace=True)\n",
    "        df_user_unique.rename({'zerobased': 'user_id'}, axis=1, inplace=True)\n",
    "        df_user_unique = df_user_unique[['user_id', 'product_id', 'interaction']]\n",
    "\n",
    "        # shuffling data for better training mechanics later\n",
    "        df_user_unique = df_user_unique.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # writing to file\n",
    "        df_user_unique.to_csv(filename, mode='a', index=False, header=(not os.path.exists(filename)))\n",
    "\n",
    "        del df_user\n",
    "        del df_user_unique\n",
    "\n",
    "    print ('done with for loop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "df979659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ground_truth(filename, user_study):\n",
    "    # filename to write ground truth instances to\n",
    "    # creating 0-based index dataframe to convert original index back more efficiently\n",
    "\n",
    "    df_user_zerobased_map = pd.DataFrame.from_dict(user_zerobased_map, orient='index', columns=['zerobased'])\n",
    "    df_user_zerobased_map['user_id'] = df_user_zerobased_map.index\n",
    "    df_user_zerobased_map.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_interaction_train_ground_truth = orders_products_train[orders_products_train.user_id.isin(user_study)][[\"user_id\", \"product_id\"]].reset_index(drop=True)\n",
    "    df_interaction_train_ground_truth['interaction'] = 1\n",
    "\n",
    "    # converting user_id and product_id back to original index\n",
    "    df_interaction_train_ground_truth.user_id = df_interaction_train_ground_truth.user_id.astype(int)\n",
    "    df_interaction_train_ground_truth = pd.merge(df_interaction_train_ground_truth, df_user_zerobased_map, how='left', on='user_id')\n",
    "    df_interaction_train_ground_truth.drop(['user_id'], axis=1, inplace=True)\n",
    "    df_interaction_train_ground_truth.rename({'zerobased': 'user_id'}, axis=1, inplace=True)\n",
    "    df_interaction_train_ground_truth = df_interaction_train_ground_truth[['user_id', 'product_id', 'interaction']]\n",
    "    df_interaction_train_ground_truth.product_id = df_interaction_train_ground_truth.product_id - 1\n",
    "\n",
    "    # saving to file\n",
    "    df_interaction_train_ground_truth.to_pickle(ground_truth_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0978fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing\n",
    "\n",
    "# n=2\n",
    "# include_hard_negative = True\n",
    "\n",
    "\n",
    "# # filtering previous purchase data for population we've selected (user_study)\n",
    "# df_interaction_prior = orders_products_prior[orders_products_prior.user_id.isin(user_study)][[\"user_id\", \"product_id\"]].reset_index(drop=True)\n",
    "# #df_interaction_prior = orders_products_prior[[\"user_id\", \"product_id\"]].reset_index(drop=True)\n",
    "\n",
    "# # specifying that our target variable is 1 for all previously purchased items\n",
    "# df_interaction_prior['interaction'] = 1\n",
    "\n",
    "# df = products[[\"product_id\"]].copy()\n",
    "\n",
    "# list_user_df = []\n",
    "# counter = 0\n",
    "# dict_all_nn = {}\n",
    "# for user_id_group in batch(df_interaction_prior.user_id.unique(), 500):\n",
    "#     counter += 500\n",
    "#     print (counter)\n",
    "#     # creating our negative training examples\n",
    "#     df_user = pd.concat([pd.concat([df.copy() for x in range(len(user_id_group))], axis=0).reset_index(drop=True), \n",
    "#                          pd.concat([pd.Series(np.ones(products.shape[0]) * user_id, name='user_id') for user_id in user_id_group], axis=0).reset_index(drop=True)], axis=1)\n",
    "#     df_user[\"interaction\"] = 0\n",
    "\n",
    "#     # concatenating positive (purchased items) and negative training examples\n",
    "#     df_user = pd.concat([df_interaction_prior[df_interaction_prior.user_id.isin(user_id_group)], df_user], \n",
    "#                             axis=0, ignore_index=True)\n",
    "\n",
    "#     # removing instances where we have a positive and negative instance for the same product. The negative instance\n",
    "#     # will be dropped and the positive example will remain\n",
    "#     df_user['distinct_count'] = df_user.groupby(by=[\"user_id\", \"product_id\"])['interaction'].transform('nunique')\n",
    "#     df_user_unique = df_user[~((df_user.interaction == 0) & (df_user.distinct_count == 2))].reset_index(drop=True)\n",
    "#     df_user_unique.drop('distinct_count', axis=1, inplace=True)\n",
    "\n",
    "#     # converting user_id and product_id back to their original index\n",
    "#     df_user_unique.user_id = df_user_unique.user_id.astype(int)\n",
    "#     df_user_unique.product_id = df_user_unique.product_id.astype(int)\n",
    "#     df_user_unique.product_id = df_user_unique.product_id - 1\n",
    "\n",
    "#     # only want to include products that are included in our top X predictions from stage 1 model\n",
    "#     df_user_unique = pd.merge(df_user_unique, cg_exploded[cg_exploded.user_id.isin(user_id_group)], how='inner', on=['user_id', 'product_id'])\n",
    "#     df_user_unique.user_id = df_user_unique.user_id.apply(lambda x: user_zerobased_map[x])\n",
    "\n",
    "#     # add in hard negatives based on parameter, n\n",
    "#     if include_hard_negative:\n",
    "        \n",
    "#         df_nn = df_user_unique.copy()\n",
    "#         df_nn = df_nn.drop_duplicates()\n",
    "#         df_nn = df_nn[df_nn.interaction == 1].copy()\n",
    "#         user_cohort_purchased_products = df_user_unique[df_user_unique.interaction == 1].product_id.unique()\n",
    "#         start = time.time()\n",
    "#         dict_all_nn = update_neighbor_dict(user_cohort_purchased_products, dict_all_nn, n)\n",
    "#         print (len(dict_all_nn.items()), 'items in dictionary.')\n",
    "#         end = time.time()\n",
    "#         print ('completed neighbor search in', np.round((end-start), 2), 'seconds.')\n",
    "        \n",
    "#         df_nn['nearest_neighbors'] = df_nn.product_id.map(dict_all_nn)\n",
    "#         df_nn = df_nn.explode('nearest_neighbors')\n",
    "#         df_nn.drop(['product_id'], axis=1, inplace=True)\n",
    "#         df_nn = df_nn.rename({'nearest_neighbors': 'product_id'}, axis=1)\n",
    "#         df_nn['interaction'] = 0\n",
    "#         df_user_unique = pd.concat([df_user_unique, df_nn], axis=0).reset_index(drop=True)\n",
    "#         df_user_unique['distinct_count'] = df_user_unique.groupby(by=[\"user_id\", \"product_id\"])['interaction'].transform('nunique')\n",
    "#         df_user_unique = df_user_unique[~((df_user_unique.interaction == 0) & (df_user_unique.distinct_count == 2))].reset_index(drop=True)\n",
    "#         df_user_unique.drop('distinct_count', axis=1, inplace=True)\n",
    "        \n",
    "# #         user_cohort_purchased_products = df_user_unique[df_user_unique.interaction == 1].product_id.unique()\n",
    "        \n",
    "# #         start = time.time()\n",
    "# #         closest_neighbor_dict = update_neighbor_dict(user_cohort_purchased_products, n)\n",
    "# #         end = time.time()\n",
    "# #         print ('completed neighbor search in', np.round((end-start), 2), 'seconds.')\n",
    "# #         dict_all_nn.update(closest_neighbor_dict)\n",
    "\n",
    "# #         df_closest_neighbors = pd.DataFrame(closest_neighbor_dict.items(), columns=['product_id', 'nearest_neighbors']).explode('nearest_neighbors').reset_index(drop=True)\n",
    "# #         df_closest_neighbors['interaction'] = 0\n",
    "# #         hard_negatives = pd.merge(df_user_unique[df_user_unique.interaction == 1].drop_duplicates(), df_closest_neighbors, how='left', on='product_id')\n",
    "# #         hard_negatives.drop(['interaction_x', 'product_id'], axis=1, inplace=True)\n",
    "# #         hard_negatives = hard_negatives.rename({'interaction_y': 'interaction', 'nearest_neighbors': 'product_id'}, axis=1)\n",
    "# #         df_user_unique = pd.concat([df_user_unique, hard_negatives], axis=0).reset_index(drop=True)\n",
    "# #         df_user_unique['distinct_count'] = df_user_unique.groupby(by=[\"user_id\", \"product_id\"])['interaction'].transform('nunique')\n",
    "# #         df_user_unique = df_user_unique[~((df_user_unique.interaction == 0) & (df_user_unique.distinct_count == 2))].reset_index(drop=True)\n",
    "# #         df_user_unique.drop('distinct_count', axis=1, inplace=True)\n",
    "    \n",
    "#     print ('null values:', df_user_unique[df_user_unique.product_id.isnull()].shape[0])\n",
    "#     print ('')\n",
    "#     print ('')\n",
    "#     # shuffling data for better training mechanics later\n",
    "#     df_user_unique = df_user_unique.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#     # writing to file\n",
    "#     df_user_unique.to_csv(filename, mode='a', index=False, header=(not os.path.exists(filename)))\n",
    "\n",
    "#     del df_user\n",
    "#     del df_user_unique\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eec864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nn = df_user_unique.copy()\n",
    "# df_nn = df_nn.drop_duplicates()\n",
    "# df_nn = df_nn[df_nn.interaction == 1].copy()\n",
    "# df_nn['nearest_neighbors'] = df_nn.product_id.map(dict_all_nn)\n",
    "# df_nn = df_nn.explode('nearest_neighbors')\n",
    "# df_nn.drop(['product_id'], axis=1, inplace=True)\n",
    "# df_nn = df_nn.rename({'nearest_neighbors': 'product_id'}, axis=1)\n",
    "# df_nn['interaction'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1ed8a7",
   "metadata": {},
   "source": [
    "# to do\n",
    "\n",
    "- pull closest neighbors to products purchased\n",
    "- make sure user has not purchased them in the past\n",
    "- add them into interaction_prior\n",
    "- BREAK\n",
    "- include code in here for validation set and ground truth set\n",
    "- set up code to start training on set\n",
    "- BREAK\n",
    "- try to include portion of randomly sampling products from entire distribution\n",
    "- include all previously purchased items as positives\n",
    "- try speed up from ANN (spotify)\n",
    "- calculate theoretical best MAP score and read up on NDCG score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc2acb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# closest_neighbor_dict = {}\n",
    "# for x in user_cohort_purchased_products:\n",
    "#     if x not in dict_all_nn:\n",
    "#         closest_neighbor_dict[x] = grab_closest_n_neighbors(2, x, tree)\n",
    "# end = time.time()\n",
    "# print (end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cfe74229",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "124057 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "1000\n",
      "142996 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "1500\n",
      "151902 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "2000\n",
      "177291 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "2500\n",
      "166384 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "3000\n",
      "172334 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "3500\n",
      "172415 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "4000\n",
      "169328 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "4500\n",
      "168505 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "5000\n",
      "172050 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "5500\n",
      "175938 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "6000\n",
      "166869 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "6500\n",
      "172341 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "7000\n",
      "167359 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "7500\n",
      "164720 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "8000\n",
      "158403 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "8500\n",
      "178261 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "9000\n",
      "158942 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "9500\n",
      "145593 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "10000\n",
      "150480 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "10500\n",
      "146721 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "11000\n",
      "148788 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "11500\n",
      "142391 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "12000\n",
      "152222 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "12500\n",
      "142458 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "13000\n",
      "140006 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "13500\n",
      "134475 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "14000\n",
      "136812 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "14500\n",
      "134066 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "15000\n",
      "122193 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "15500\n",
      "115970 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "16000\n",
      "111913 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "16500\n",
      "118243 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "17000\n",
      "109963 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "17500\n",
      "106957 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "18000\n",
      "82732 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "18500\n",
      "91268 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "19000\n",
      "82109 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "19500\n",
      "74290 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "20000\n",
      "65312 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "20500\n",
      "42606 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "21000\n",
      "4290 negatives were added from random negative sampling.\n",
      "null values: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filename = \"interaction_prior_20k_rank250_100recs.csv\"\n",
    "# train_data(filename, orders_products_prior, products, user_study)\n",
    "# train_data(filename, orders_products_prior, products, user_study, True, nn_option)\n",
    "\n",
    "\n",
    "filename = \"interaction_prior_20k_rank250_100recs_withrns(5).csv\"\n",
    "# train_data(filename, orders_products_prior, products, user_study, include_hard_negative, n_hard_negatives, all_positives, include_random_popularity_sampling, n_rns)\n",
    "train_data(filename, orders_products_prior, products, user_study, False, 1, False, True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a116a03",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "500\n",
      "2788 items in dictionary.\n",
      "completed neighbor search in 30.52 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "1000\n",
      "4345 items in dictionary.\n",
      "completed neighbor search in 18.88 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "1500\n",
      "5252 items in dictionary.\n",
      "completed neighbor search in 10.82 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "2000\n",
      "5923 items in dictionary.\n",
      "completed neighbor search in 8.06 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "2500\n",
      "6510 items in dictionary.\n",
      "completed neighbor search in 7.28 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "3000\n",
      "6979 items in dictionary.\n",
      "completed neighbor search in 5.64 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "3500\n",
      "7344 items in dictionary.\n",
      "completed neighbor search in 4.36 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "4000\n",
      "7701 items in dictionary.\n",
      "completed neighbor search in 4.19 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "4500\n",
      "8024 items in dictionary.\n",
      "completed neighbor search in 4.05 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "5000\n",
      "8289 items in dictionary.\n",
      "completed neighbor search in 3.17 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "5500\n",
      "8573 items in dictionary.\n",
      "completed neighbor search in 3.38 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "6000\n",
      "8816 items in dictionary.\n",
      "completed neighbor search in 2.8 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "6500\n",
      "9043 items in dictionary.\n",
      "completed neighbor search in 2.71 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "7000\n",
      "9228 items in dictionary.\n",
      "completed neighbor search in 2.13 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "7500\n",
      "9444 items in dictionary.\n",
      "completed neighbor search in 2.78 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "8000\n",
      "9700 items in dictionary.\n",
      "completed neighbor search in 3.11 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "8500\n",
      "9999 items in dictionary.\n",
      "completed neighbor search in 3.54 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "9000\n",
      "10269 items in dictionary.\n",
      "completed neighbor search in 3.14 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "9500\n",
      "10558 items in dictionary.\n",
      "completed neighbor search in 3.45 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "10000\n",
      "10669 items in dictionary.\n",
      "completed neighbor search in 1.4 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "10500\n",
      "10765 items in dictionary.\n",
      "completed neighbor search in 1.23 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "11000\n",
      "10872 items in dictionary.\n",
      "completed neighbor search in 1.24 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "11500\n",
      "10968 items in dictionary.\n",
      "completed neighbor search in 1.13 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "12000\n",
      "11108 items in dictionary.\n",
      "completed neighbor search in 1.97 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "12500\n",
      "11284 items in dictionary.\n",
      "completed neighbor search in 2.03 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "13000\n",
      "11541 items in dictionary.\n",
      "completed neighbor search in 2.91 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "13500\n",
      "11749 items in dictionary.\n",
      "completed neighbor search in 2.41 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "14000\n",
      "12056 items in dictionary.\n",
      "completed neighbor search in 3.47 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "14500\n",
      "12329 items in dictionary.\n",
      "completed neighbor search in 3.22 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "15000\n",
      "12487 items in dictionary.\n",
      "completed neighbor search in 2.09 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "15500\n",
      "12703 items in dictionary.\n",
      "completed neighbor search in 2.97 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "16000\n",
      "12868 items in dictionary.\n",
      "completed neighbor search in 1.83 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "16500\n",
      "13116 items in dictionary.\n",
      "completed neighbor search in 2.92 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "17000\n",
      "13319 items in dictionary.\n",
      "completed neighbor search in 2.34 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "17500\n",
      "13535 items in dictionary.\n",
      "completed neighbor search in 2.63 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "18000\n",
      "13612 items in dictionary.\n",
      "completed neighbor search in 0.85 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "18500\n",
      "13809 items in dictionary.\n",
      "completed neighbor search in 2.52 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "19000\n",
      "13960 items in dictionary.\n",
      "completed neighbor search in 1.71 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "19500\n",
      "14119 items in dictionary.\n",
      "completed neighbor search in 1.81 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "20000\n",
      "14278 items in dictionary.\n",
      "completed neighbor search in 1.9 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "20500\n",
      "14395 items in dictionary.\n",
      "completed neighbor search in 1.32 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n",
      "21000\n",
      "14418 items in dictionary.\n",
      "completed neighbor search in 0.26 seconds.\n",
      "null values: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for nn_option in [10]:\n",
    "    print (nn_option)\n",
    "    filename = \"interaction_prior_20k_rank250_100recs_with_hardnegatives_\" + str(nn_option)+\"neighbors.csv\"\n",
    "    train_data(filename, orders_products_prior, products, user_study, True, nn_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec002d45",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction_prior_20k_rank250_100recs_with_hardnegatives_3neighbors.csv\n"
     ]
    }
   ],
   "source": [
    "filename = \"interaction_prior_20k_rank250_100recs_with_hardnegatives_2neighbors.csv\"\n",
    "train_data(filename, orders_products_prior, products, user_study, True, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda30af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"interaction_train_20k_rank250_100recs.csv\"\n",
    "val_data(filename, orders_products_train, products, user_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_filename = \"cg_interaction_ground_truth_20k_rank250_100recs.pkl\"\n",
    "extract_ground_truth(ground_truth_filename, user_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18738bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38904e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector_list = products.features.tolist()\n",
    "# index = 4\n",
    "# study_index_vector = vector_list[index]\n",
    "\n",
    "# print (products.loc[index, 'product_name'])\n",
    "# #mod_vector_list = vector_list[:index] + vector_list[index+1:]\n",
    "# # vector_list[index] = np.ones(100) * 20\n",
    "# tree = spatial.KDTree(vector_list)\n",
    "\n",
    "# tree_search_indices = tree.query(study_index_vector, k=20)[1]\n",
    "# print (tree_search_indices)\n",
    "# products.loc[tree_search_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e225a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
