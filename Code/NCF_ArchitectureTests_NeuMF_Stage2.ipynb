{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62050477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras --upgrade\n",
    "#!pip uninstall tensorflow\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cdf4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "658476a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.6.0\n",
      "LightFM version: 1.16\n",
      "Pandas version: 1.4.1\n",
      "Numpy version: 1.19.5\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Merge' from 'tensorflow.keras.layers' (/opt/conda/lib/python3.8/site-packages/keras/api/_v2/keras/layers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19420/260856225.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m from tensorflow.keras.layers import (\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mConcatenate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Merge' from 'tensorflow.keras.layers' (/opt/conda/lib/python3.8/site-packages/keras/api/_v2/keras/layers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# !pip install tensorflow lightfm pandas\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import datetime\n",
    "import lightfm\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from lightfm import LightFM\n",
    "# from lightfm.datasets import fetch_movielens\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "from __future__ import print_function  # for Python2\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "\n",
    "import gc\n",
    "# hide\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "print(f\"LightFM version: {lightfm.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import (\n",
    "    Concatenate,\n",
    "    Dense,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    Multiply,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    "    Dot,\n",
    "    Merge\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# from numba import cuda \n",
    "# device = cuda.get_current_device()\n",
    "# device.reset()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print (\"gpus:\", gpus)\n",
    "\n",
    "### Configurations\n",
    "\n",
    "# Configurations\n",
    "TOP_K = 15\n",
    "\n",
    "products = pd.read_csv('products.csv')\n",
    "\n",
    "# Define multi-gpu strategy \n",
    "BATCHSIZE= 512\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "print (mirrored_strategy.num_replicas_in_sync, 'GPUs in Use')\n",
    "# Update batch size value\n",
    "BATCHSIZE = BATCHSIZE * mirrored_strategy.num_replicas_in_sync\n",
    "\n",
    "sys_details = tf.sysconfig.get_build_info()\n",
    "cuda_version = sys_details[\"cuda_version\"]\n",
    "print(cuda_version)\n",
    "cudnn_version = sys_details[\"cudnn_version\"]  \n",
    "print(cudnn_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9254a633",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584549af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9892b595",
   "metadata": {},
   "source": [
    "### Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ecd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv(\"DLS_Project/interaction_prior_20k_rank250_100recs_with_hardnegatives_2neighbors.csv\")\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight(\n",
    "#                'balanced',\n",
    "#                 np.unique(df_train.interaction.tolist()), \n",
    "#                 df_train.interaction.tolist()) \n",
    "\n",
    "# train_user_n, train_product_n = df_train.user_id.nunique(), 49688\n",
    "# print (df_train.shape)\n",
    "# print (class_weights)\n",
    "# # print (df_train.interaction.value_counts(normalize=True))\n",
    "# # print (train_user_n)\n",
    "# # df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efee3afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5860fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b9afc",
   "metadata": {},
   "source": [
    "### Reading in Instacart Feature Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb00c39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49688, 100)\n"
     ]
    }
   ],
   "source": [
    "df_instacart_product_vectors = pd.read_csv(\"instacart_feature_vec.csv\", header=None)\n",
    "\n",
    "df_instacart_product_vectors.head()\n",
    "\n",
    "df_instacart_product_vectors = df_instacart_product_vectors.drop(101, axis=1)\n",
    "\n",
    "df_instacart_product_vectors = df_instacart_product_vectors.sort_values(0)\n",
    "\n",
    "df_instacart_product_vectors = df_instacart_product_vectors.drop(0, axis=1)\n",
    "\n",
    "np_instacart_product_vectors = df_instacart_product_vectors.to_numpy()\n",
    "\n",
    "np_instacart_product_vectors = np.insert(np_instacart_product_vectors, 0, np.zeros(100), 0)\n",
    "\n",
    "print (np_instacart_product_vectors.shape)\n",
    "\n",
    "# only if we're using negative sampling\n",
    "# np_instacart_product_vectors = np.take(np_instacart_product_vectors, w2v_indices, axis=0)\n",
    "\n",
    "# df_train.product_id.nunique()\n",
    "# del df_instacart_product_vectors\n",
    "# gc.collect()\n",
    "# df_instacart_product_vectors = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4835e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# products['features'] = pd.Series(np_instacart_product_vectors.tolist())\n",
    "\n",
    "# products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e1942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading in Pre-Trained MF Embeddings\n",
    "\n",
    "# file_name = 'user_embedding_MF_ALS.pkl'\n",
    "# open_file = open(file_name, \"rb\")\n",
    "# user_embeddings = pickle.load(open_file)\n",
    "# open_file.close()\n",
    "\n",
    "# file_name = 'product_embedding_MF_ALS.pkl'\n",
    "# open_file = open(file_name, \"rb\")\n",
    "# product_embeddings = pickle.load(open_file)\n",
    "# open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695655c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (type(user_embeddings), user_embeddings.shape)\n",
    "# print (type(product_embeddings), product_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d180df5",
   "metadata": {},
   "source": [
    "# To Do\n",
    "\n",
    "- Figure out why there's 49687 unique products in training set while there are actually 49688 unique products in total\n",
    "- Try calibration layer / regression on top of negative sampling NeuFM\n",
    "- Run just MF save embeddings and use embedding look up to freeze weights, while training NN side of network with negative sampling\n",
    "- Once this is done, we move to optimizing the NN with various hyperparameters\n",
    "- figure out how to use tensorboard\n",
    "- trying to get to 0.21, 0.22 MAP value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_instacart_product_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f337fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run if randomized index\n",
    "\n",
    "# user_to_zerobased_index = dict(zip(df_train.user_id.unique(), range(df_train.user_id.nunique())))\n",
    "# product_to_zerobased_index = dict(zip(df_train.product_id.unique(), range(df_train.product_id.nunique())))\n",
    "\n",
    "# zerobased_index_to_product = {value: key for key, value in product_to_zerobased_index.items()}\n",
    "\n",
    "# zerobased_index_to_user = {value: key for key, value in user_to_zerobased_index.items()}\n",
    "\n",
    "# df_train.user_id = df_train.user_id.apply(lambda x: user_to_zerobased_index[x])\n",
    "# df_train.product_id = df_train.product_id.apply(lambda x: product_to_zerobased_index[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "866a7645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sort the list by second item of tuple\n",
    "def Sort_Tuple(tup): \n",
    "  \n",
    "    # reverse = None (Sorts in Ascending order) \n",
    "    # key is set to sort using second element of \n",
    "    # sublist lambda has been used \n",
    "    return(sorted(tup, key = lambda x: x[1], reverse=True))\n",
    "\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def make_tf_dataset_train(\n",
    "    df: pd.DataFrame,\n",
    "    targets: List[str],\n",
    "    val_split: float = 0.2,\n",
    "    batch_size: int = BATCHSIZE,\n",
    "    seed=28,\n",
    "):\n",
    "    \"\"\"Make TensorFlow dataset from Pandas DataFrame.\n",
    "    :param df: input DataFrame - only contains features and target(s)\n",
    "    :param targets: list of columns names corresponding to targets\n",
    "    :param val_split: fraction of the data that should be used for validation\n",
    "    :param batch_size: batch size for training\n",
    "    :param seed: random seed for shuffling data - `None` won't shuffle the data\"\"\"\n",
    "\n",
    "    n_val = round(df.shape[0] * val_split)\n",
    "    if seed:\n",
    "        # shuffle all the rows\n",
    "        x = df.sample(frac=1, random_state=seed).to_dict(\"series\")\n",
    "    else:\n",
    "        x = df.to_dict(\"series\")\n",
    "    y = dict()\n",
    "    for t in targets:\n",
    "        y[t] = x.pop(t)\n",
    "    interaction_terms = y[t].values\n",
    "    sample_weight = np.ones(shape=(len(interaction_terms),))\n",
    "    #sample_weight[interaction_terms == 0] = 2.037969065266319\n",
    "    #sample_weight[interaction_terms == 1] = 0.6625520341377668\n",
    "    sample_weight[interaction_terms == 0] = class_weights[0]\n",
    "    sample_weight[interaction_terms == 1] = class_weights[1]\n",
    "#     print (sample_weight)\n",
    "#         y[t] = tf.keras.utils.to_categorical(x.pop(t))\n",
    "#     print (y.values())\n",
    "#     print (x.values())\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y, sample_weight))\n",
    "    \n",
    "    #ds = ds.cache()\n",
    "    \n",
    "    ds_val = ds.take(n_val).batch(batch_size)\n",
    "    #ds_val = ds_val.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    ds_train = ds.skip(n_val).batch(batch_size)\n",
    "    #ds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return ds_train, ds_val\n",
    "\n",
    "def make_tf_dataset_val(\n",
    "    df: pd.DataFrame,\n",
    "    targets: List[str],\n",
    "    val_split: float = 0.1,\n",
    "    batch_size: int = BATCHSIZE,\n",
    "    seed=28,\n",
    "):\n",
    "    \"\"\"Make TensorFlow dataset from Pandas DataFrame.\n",
    "    :param df: input DataFrame - only contains features and target(s)\n",
    "    :param targets: list of columns names corresponding to targets\n",
    "    :param val_split: fraction of the data that should be used for validation\n",
    "    :param batch_size: batch size for training\n",
    "    :param seed: random seed for shuffling data - `None` won't shuffle the data\"\"\"\n",
    "\n",
    "    n_val = round(df.shape[0] * val_split)\n",
    "    if seed:\n",
    "        # shuffle all the rows\n",
    "        x = df.sample(frac=1, random_state=seed).to_dict(\"series\")\n",
    "    else:\n",
    "        x = df.to_dict(\"series\")\n",
    "    y = dict()\n",
    "    for t in targets:\n",
    "        y[t] = x.pop(t)\n",
    "\n",
    "#         y[t] = tf.keras.utils.to_categorical(x.pop(t))\n",
    "#     print (y.values())\n",
    "#     print (x.values())\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    \n",
    "    #ds = ds.cache()\n",
    "    \n",
    "    ds_val = ds.take(n_val).batch(batch_size)\n",
    "    #ds_val = ds_val.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    ds_train = ds.skip(n_val).batch(batch_size)\n",
    "    #ds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return ds_train, ds_val\n",
    "\n",
    "def grab_memory_usage(local_var):\n",
    "    vars_list = []\n",
    "    sizes = []\n",
    "\n",
    "    local_vars = list(local_var.items())\n",
    "    for var, obj in local_vars:\n",
    "        vars_list.append(var)\n",
    "        sizes.append(sys.getsizeof(obj))\n",
    "        #print(var, sys.getsizeof(obj))\n",
    "\n",
    "    df_var_sizes = pd.DataFrame([vars_list, sizes]).T\n",
    "    df_var_sizes.columns = ['variable', 'sizes']\n",
    "    df_var_sizes = df_var_sizes.sort_values(\"sizes\", ascending=False).reset_index(drop=True)\n",
    "    df_var_sizes.sizes = df_var_sizes.sizes/1000000000.00\n",
    "    return df_var_sizes\n",
    "\n",
    "# grab_memory_usage(locals())\n",
    "\n",
    "class MemoryPrintingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        #gpu_dict = tf.config.experimental.get_memory_info('GPU:0')\n",
    "        gpu_usage = tf.config.experimental.get_memory_usage('GPU:0')\n",
    "        tf.print('\\n GPU memory details [current: {} gb]'.format(\n",
    "          float(gpu_usage) / (1024 ** 3)))\n",
    "#         tf.print('\\n GPU memory details [current: {} gb, peak: {} gb]'.format(\n",
    "#           float(gpu_dict['current']) / (1024 ** 3), \n",
    "#           float(gpu_dict['peak']) / (1024 ** 3)))\n",
    "\n",
    "# tf.config.experimental.get_memory_usage('GPU:0') / (1024**3)\n",
    "\n",
    "# Saving Model\n",
    "\n",
    "def save_model_artifacts(train_hist, ncf_model, train_filename, model_architecture, dropout_input,dropout_hidden_layer, dropout_on_multiplication_boolean, dropout_on_multiplication,\n",
    "                        latent_dim_mf, latent_dim_mlp, reg_mf, reg_mlp, dense_layers, reg_layers, N_EPOCHS, BATCHSIZE):\n",
    "\n",
    "    pd.DataFrame.from_dict(train_hist.history).to_csv(str(train_filename) + \"_\" + str(model_architecture) + \"_\" + str(dropout_input) + \"_\" + str(dropout_hidden_layer) +\n",
    "                                                     \"_\" + str(dropout_on_multiplication_boolean) + \"_\" + str(dropout_on_multiplication) + \"_\" + str(latent_dim_mlp) + \"_\" + str(latent_dim_mf) +\n",
    "                                                     \"_\" + str(reg_mf) + \"_\" + str(reg_mlp) + \"_\" + str(dense_layers) + \"_\" + str(reg_layers) + \"_\" + str(N_EPOCHS) + \"_\" + str(BATCHSIZE) + \"_\" + str(batch_norm_boolean) + \"_\" + str(dropout_on_multiplication_boolean) \n",
    "                                                      + \"_\" + str(dropout_on_multiplication) + \".csv\")\n",
    "    \n",
    "#     ncf_model.save(\"~/work/DLS_Project/model_\" + str(train_filename) + \"_\" + str(model_architecture) + \"_\" + str(dropout_input) + \"_\" + str(dropout_hidden_layer) +\n",
    "#                                                      \"_\" + str(dropout_on_multiplication_boolean) + \"_\" + str(dropout_on_multiplication) + \"_\" + str(latent_dim_mlp) + \"_\" + str(latent_dim_mf) +\n",
    "#                                                      \"_\" + str(reg_mf) + \"_\" + str(reg_mlp) + \"_\" + str(dense_layers) + \"_\" + str(reg_layers))   \n",
    "\n",
    "\n",
    "# Loading Validation and Making Predictions\n",
    "\n",
    "def load_val_make_preds(ncf_model, val_filename, gt_filename):\n",
    "\n",
    "    ### Load Validation Set\n",
    "\n",
    "    df_val = pd.read_csv(val_filename)\n",
    "    df_truth = pd.read_pickle(gt_filename)\n",
    "    ds_test, _ = make_tf_dataset_val(df_val, ['interaction'], val_split=0, seed=None)\n",
    "    ncf_predictions = ncf_model.predict(ds_test, batch_size=512, max_queue_size=500, verbose=0)\n",
    "    df_val[\"ncf_predictions\"] = ncf_predictions\n",
    "    df_val[\"pred\"] = (df_val.ncf_predictions >= 0.5) * 1.0\n",
    "    df_val[\"correct\"] = (df_val.interaction == df_val.pred) * 1.0\n",
    "    df_val.drop_duplicates(inplace=True)\n",
    "    return df_val, df_truth\n",
    "\n",
    "def evaluate_and_save_results(df_val, df_truth):\n",
    "#     std = df_val.ncf_predictions.std()\n",
    "#     if std < 0.01:\n",
    "#         raise ValueError(\"Model predictions have standard deviation of less than 1e-2.\")\n",
    "\n",
    "    all_indices_predictions = df_val.user_id.unique()\n",
    "    all_indices_predictions.sort()\n",
    "\n",
    "    sorted_df_val = df_val[['user_id', 'product_id', 'ncf_predictions']].sort_values(by=['user_id', 'ncf_predictions'], ascending=[True, False])\n",
    "    gb_sorted_df_val = sorted_df_val.groupby('user_id')\n",
    "    gb_df_truth = df_truth.groupby('user_id')\n",
    "\n",
    "    predictionAndLabelsAll = []\n",
    "    counter = 0\n",
    "    for i in all_indices_predictions:\n",
    "        if counter % 5000 == 0:\n",
    "            print (counter)\n",
    "        counter+=1 \n",
    "        preds = gb_sorted_df_val.get_group(i).product_id.tolist()\n",
    "        truth = gb_df_truth.get_group(i).product_id.tolist()\n",
    "        predictionAndLabelsAll.append((preds, truth))\n",
    "\n",
    "    predictionAndLabels_All = sc.parallelize(predictionAndLabelsAll)\n",
    "    metrics_all = RankingMetrics(predictionAndLabels_All)\n",
    "    \n",
    "    return (metrics_all.meanAveragePrecisionAt(15), metrics_all.ndcgAt(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8564447",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0477eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mirrored_strategy.scope():\n",
    "    \n",
    "    def lr_scheduler(epoch, lr):\n",
    "        if epoch > 1:\n",
    "            lr = 0.0005\n",
    "        if epoch > 3:\n",
    "            lr = 0.0001\n",
    "            return lr\n",
    "        if epoch > 5:\n",
    "            lr = 0.00005\n",
    "        return lr\n",
    "    \n",
    "    def generate_embedding(input_dim, output_dim, name, reg_mf):\n",
    "        return Embedding(\n",
    "                input_dim=input_dim,\n",
    "                output_dim=output_dim,\n",
    "                name=name,\n",
    "                embeddings_initializer=\"RandomNormal\",\n",
    "                embeddings_regularizer=l2(reg_mf),\n",
    "                input_length=1,\n",
    "            )\n",
    "    \n",
    "    def create_ncf(\n",
    "        number_of_users: int,\n",
    "        number_of_items: int,\n",
    "        latent_dim_mf: int = 32,\n",
    "        latent_dim_mlp: int = 64,\n",
    "        reg_mf: int = 0,\n",
    "        reg_mlp: int = 0.001,\n",
    "        dense_layers: List[int] = [1024, 512, 256],\n",
    "        reg_layers: List[int] = [0.001, 0.001, 0.001],\n",
    "        activation_dense: str = \"relu\",\n",
    "    ) -> keras.Model:\n",
    "\n",
    "        # input layer\n",
    "        user = Input(shape=(), dtype=\"int32\", name=\"user_id\")\n",
    "        item = Input(shape=(), dtype=\"int32\", name=\"product_id\")\n",
    "\n",
    "\n",
    "        embedding_dim = 100\n",
    "        num_tokens = n_items\n",
    "\n",
    "        product_embedding_layer = Embedding(\n",
    "            num_tokens,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=keras.initializers.Constant(np_instacart_product_vectors),\n",
    "            #embeddings_initializer=keras.initializers.VarianceScaling(np_instacart_product_vectors),\n",
    "            trainable=False,\n",
    "            input_length=1,\n",
    "        )\n",
    "\n",
    "    #     product_mf_embedding_dim = 250\n",
    "    #     num_tokens_products = train_product_n\n",
    "\n",
    "    #     pretrained_product_embedding_mf = Embedding(\n",
    "    #         num_tokens_products,\n",
    "    #         product_mf_embedding_dim,\n",
    "    #         embeddings_initializer=keras.initializers.Constant(product_embeddings),\n",
    "    #         #embeddings_initializer=keras.initializers.VarianceScaling(np_instacart_product_vectors),\n",
    "    #         trainable=False,\n",
    "    #         input_length=1,\n",
    "    #     )\n",
    "\n",
    "    #     user_mf_embedding_dim = 250\n",
    "    #     num_tokens_users = train_user_n\n",
    "\n",
    "    #     pretrained_user_embedding_mf = Embedding(\n",
    "    #         num_tokens_users,\n",
    "    #         user_mf_embedding_dim,\n",
    "    #         embeddings_initializer=keras.initializers.Constant(user_embeddings),\n",
    "    #         #embeddings_initializer=keras.initializers.VarianceScaling(np_instacart_product_vectors),\n",
    "    #         trainable=False,\n",
    "    #         input_length=1,\n",
    "    #     )\n",
    "\n",
    "        #embedding layers\n",
    "        \n",
    "        mf_user_embedding = generate_embedding(number_of_users, latent_dim_mf, \"mf_user_embedding\", reg_mf)\n",
    "        mf_item_embedding = generate_embedding(number_of_items, latent_dim_mf, \"mf_item_embedding\", reg_mf)\n",
    "        \n",
    "        mlp_user_embedding = generate_embedding(number_of_users, latent_dim_mlp, \"mlp_user_embedding\", reg_mlp)\n",
    "        mlp_item_embedding = generate_embedding(number_of_items, latent_dim_mlp, \"mlp_item_embedding\", reg_mlp)\n",
    "        \n",
    "        mlp_user_embedding_wv = generate_embedding(number_of_users, latent_dim_mlp, \"mlp_user_embedding\", reg_mlp)\n",
    "        mlp_item_embedding_wv = generate_embedding(number_of_items, latent_dim_mlp, \"mlp_item_embedding\", reg_mlp)\n",
    "\n",
    "        # word2vec embeddings\n",
    "        product_vector = Flatten()(product_embedding_layer(item))\n",
    "\n",
    "        # Pretrained MF embeddings\n",
    "\n",
    "    #     pretrained_mf_item_latent = Flatten()(pretrained_product_embedding_mf(item))\n",
    "    #     pretrained_mf_user_latent = Flatten()(pretrained_user_embedding_mf(user))\n",
    "    #     mf_cat_latent = Multiply()([pretrained_mf_user_latent, pretrained_mf_item_latent])\n",
    "\n",
    "        # MF vector\n",
    "\n",
    "        mf_user_latent = Flatten()(mf_user_embedding(user))\n",
    "        mf_item_latent = Flatten()(mf_item_embedding(item))\n",
    "        mf_cat_latent = Multiply()([mf_user_latent, mf_item_latent])\n",
    "\n",
    "        # MLP vector 1\n",
    "        mlp_user_latent = Flatten()(mlp_user_embedding(user))\n",
    "        mlp_item_latent = Flatten()(mlp_item_embedding(item))\n",
    "        \n",
    "        # MLP vector 2\n",
    "        mlp_user_latent_wv = Flatten()(mlp_user_embedding_wv(user))\n",
    "        mlp_item_latent_wv = Flatten()(mlp_item_embedding_wv(item))\n",
    "\n",
    "\n",
    "        # MLP vector 1 Concatenation\n",
    "        \n",
    "        mlp_cat_latent = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "\n",
    "        # MLP vector 2 Concatenation\n",
    "\n",
    "        mlp_cat_latent_wv = Concatenate()([mlp_user_latent_wv, product_vector])\n",
    "        \n",
    "#         mlp_user_w2v = Concatenate()([mlp_user_latent, product_vector])\n",
    "#         mlp_item_w2v = Concatenate()([mlp_item_latent, product_vector])\n",
    "#         mlp_3pronged = Concatenate()([mlp_user_latent, product_vector, mlp_item_latent])\n",
    "#         mlp_cat_latent = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "#         mlp_cat_latent = Concatenate()([mlp_user_w2v, mlp_item_w2v])\n",
    "        \n",
    "        mlp_vector = mlp_cat_latent\n",
    "        \n",
    "    \n",
    "        # Start of NN Architecture\n",
    "    \n",
    "        mlp_vector = Dropout(0.5)(mlp_vector)\n",
    "        relu_layer = tf.keras.layers.ReLU()\n",
    "        # build dense layers for model\n",
    "        for i in range(len(dense_layers)):\n",
    "            layer = Dense(\n",
    "                dense_layers[i],\n",
    "                activity_regularizer=l2(reg_layers[i]),\n",
    "#                 activation=activation_dense,\n",
    "                activation='linear',\n",
    "                name=\"layer%d\" % i,\n",
    "            )\n",
    "            mlp_vector = layer(mlp_vector)\n",
    "            mlp_vector = relu_layer(mlp_vector)\n",
    "            #mlp_vector = BatchNormalization()(mlp_vector)\n",
    "            mlp_vector = Dropout(0.2)(mlp_vector)\n",
    "\n",
    "    #     # build dense layers for model\n",
    "    #     for i in range(len(dense_layers)):\n",
    "    #         layer = Dense(\n",
    "    #             dense_layers[i],\n",
    "    #             activity_regularizer=l2(reg_layers[i]),\n",
    "    #             activation=activation_dense,\n",
    "    #             name=\"wv_layer%d\" % i,\n",
    "    #         )\n",
    "    #         mlp_vector_wv = layer(mlp_vector_wv)  \n",
    "\n",
    "        #predict_layer = Concatenate()([mf_cat_latent, mlp_vector])\n",
    "        predict_layer = mlp_vector\n",
    "        #predict_layer = Concatenate()([mf_cat_latent, mlp_vector, product_vector])\n",
    "\n",
    "        result = Dense(\n",
    "            1, activation=\"sigmoid\", kernel_initializer=\"lecun_uniform\", name=\"interaction\"\n",
    "        )\n",
    "    \n",
    "        #predict_layer_post_dense = intermediate_result(predict_layer)\n",
    "        output = result(predict_layer)\n",
    "        #output = Flatten()(result(predict_layer))\n",
    "        #output = result(mf_cat_latent)\n",
    "\n",
    "        model = Model(\n",
    "            inputs=[user, item],\n",
    "            outputs=[output],\n",
    "        )\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9053756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tf_traindataset\n",
    "\n",
    "\n",
    "# ds_train, ds_val = make_tf_dataset_train(df_train, ['interaction'])\n",
    "# del df_train\n",
    "# gc.collect()\n",
    "# df_train = pd.DataFrame()\n",
    "\n",
    "# n_users, n_items = train_user_n, train_product_n\n",
    "# with mirrored_strategy.scope():\n",
    "\n",
    "#     print (\"batchsize:\", BATCHSIZE)\n",
    "#     # policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "#     # tf.keras.mixed_precision.experimental.set_policy(policy) \n",
    "\n",
    "#     ncf_model = create_ncf(n_users, n_items)\n",
    "\n",
    "#     ncf_model.compile(\n",
    "#         optimizer=Adam(learning_rate=0.001),\n",
    "# #         loss=\"binary_crossentropy\",\n",
    "#         loss='binary_crossentropy',\n",
    "#         #loss=\"hinge\",\n",
    "#         metrics=[\n",
    "#             tf.keras.metrics.TruePositives(name=\"tp\"),\n",
    "#             tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
    "#             tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "#             tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "#             tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "#             tf.keras.metrics.Precision(name=\"precision\"),\n",
    "#             tf.keras.metrics.Recall(name=\"recall\"),\n",
    "#             tf.keras.metrics.AUC(name=\"auc\")\n",
    "#         ],\n",
    "#         weighted_metrics=[tf.keras.metrics.BinaryAccuracy(name=\"weighted_accuracy\"),\n",
    "#             tf.keras.metrics.Precision(name=\"weighted_precision\"),\n",
    "#             tf.keras.metrics.Recall(name=\"weighted_recall\"),\n",
    "#             tf.keras.metrics.AUC(name=\"weighted_auc\")]\n",
    "#     )\n",
    "#     ncf_model._name = \"neural_collaborative_filtering\"\n",
    "#     #print (ncf_model.summary())\n",
    "#     logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "#     tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, profile_batch=(1,2))\n",
    "#     early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "#         monitor=\"val_precision\", patience=2, restore_best_weights = True,\n",
    "#     )\n",
    "    \n",
    "#     lr_scheduler_callback = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "#     train_hist = ncf_model.fit(\n",
    "#         ds_train,\n",
    "#         validation_data=ds_val,\n",
    "#         epochs=N_EPOCHS,\n",
    "#         #callbacks=[tensorboard_callback, early_stopping_callback],\n",
    "#         #callbacks=[tensorboard_callback, lr_scheduler_callback, MemoryPrintingCallback()],\n",
    "#         callbacks=[tensorboard_callback],\n",
    "#         verbose=1,\n",
    "#         shuffle=True,\n",
    "#         workers=12,\n",
    "#         use_multiprocessing=True,\n",
    "#         max_queue_size=500,\n",
    "\n",
    "#     )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c14128",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9 --> 1, 0.09 --> 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1e592242",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mirrored_strategy.scope():\n",
    "    \n",
    "    def lr_scheduler(epoch, lr):\n",
    "        if epoch > 1:\n",
    "            lr = 0.0001\n",
    "        if epoch > 3:\n",
    "            lr = 0.00009\n",
    "        if epoch > 7:\n",
    "            lr = 0.0005\n",
    "        if epoch > 9:\n",
    "            lr = 0.0003\n",
    "        if epoch > 12:\n",
    "            lr = 0.0001\n",
    "        if epoch > 30:\n",
    "            lr = 0.00009\n",
    "        if epoch > 35:\n",
    "            lr = 0.00007\n",
    "        return lr\n",
    "    \n",
    "    def generate_embedding(input_dim, output_dim, name, reg_mf):\n",
    "        return Embedding(\n",
    "                input_dim=input_dim,\n",
    "                output_dim=output_dim,\n",
    "                name=name,\n",
    "                embeddings_initializer=\"RandomNormal\",\n",
    "                embeddings_regularizer=l2(reg_mf),\n",
    "                input_length=1,\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def create_ncf(\n",
    "        architecture,\n",
    "        dropout_input,\n",
    "        dropout_hidden_layer,\n",
    "        dropout_on_multiplication_boolean,\n",
    "        dropout_on_multiplication,\n",
    "        batch_norm_boolean,\n",
    "        number_of_users: int,\n",
    "        number_of_items: int,\n",
    "        latent_dim_mf: int = 32,\n",
    "        latent_dim_mlp: int = 64,\n",
    "        reg_mf: int = 0,\n",
    "        reg_mlp: int = 0.001,\n",
    "        dense_layers: List[int] = [1024, 512, 256],\n",
    "        reg_layers: List[int] = [0.001, 0.001, 0.001],\n",
    "        activation_dense: str = \"relu\"\n",
    "        \n",
    "    ) -> keras.Model:\n",
    "\n",
    "        # input layer\n",
    "        user = Input(shape=(), dtype=\"int32\", name=\"user_id\")\n",
    "        item = Input(shape=(), dtype=\"int32\", name=\"product_id\")\n",
    "        \n",
    "        embedding_dim = 100\n",
    "        num_tokens = number_of_items\n",
    "\n",
    "        product_embedding_layer = Embedding(\n",
    "            num_tokens,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=keras.initializers.Constant(np_instacart_product_vectors),\n",
    "            #embeddings_initializer=keras.initializers.VarianceScaling(np_instacart_product_vectors),\n",
    "            trainable=False,\n",
    "            input_length=1,\n",
    "            name='w2v_product_vector'\n",
    "        )\n",
    "\n",
    "    #     product_mf_embedding_dim = 250\n",
    "    #     num_tokens_products = train_product_n\n",
    "\n",
    "    #     pretrained_product_embedding_mf = Embedding(\n",
    "    #         num_tokens_products,\n",
    "    #         product_mf_embedding_dim,\n",
    "    #         embeddings_initializer=keras.initializers.Constant(product_embeddings),\n",
    "    #         #embeddings_initializer=keras.initializers.VarianceScaling(np_instacart_product_vectors),\n",
    "    #         trainable=False,\n",
    "    #         input_length=1,\n",
    "    #     )\n",
    "\n",
    "    #     user_mf_embedding_dim = 250\n",
    "    #     num_tokens_users = train_user_n\n",
    "\n",
    "    #     pretrained_user_embedding_mf = Embedding(\n",
    "    #         num_tokens_users,\n",
    "    #         user_mf_embedding_dim,\n",
    "    #         embeddings_initializer=keras.initializers.Constant(user_embeddings),\n",
    "    #         #embeddings_initializer=keras.initializers.VarianceScaling(np_instacart_product_vectors),\n",
    "    #         trainable=False,\n",
    "    #         input_length=1,\n",
    "    #     )\n",
    "\n",
    "        #embedding layers\n",
    "        \n",
    "        mf_user_embedding = generate_embedding(number_of_users, latent_dim_mf, \"mf_user_embedding\", reg_mf)\n",
    "        mf_item_embedding = generate_embedding(number_of_items, latent_dim_mf, \"mf_item_embedding\", reg_mf)\n",
    "        \n",
    "        mlp_user_embedding = generate_embedding(number_of_users, latent_dim_mlp, \"mlp_user_embedding\", reg_mlp)\n",
    "        mlp_item_embedding = generate_embedding(number_of_items, latent_dim_mlp, \"mlp_item_embedding\", reg_mlp)\n",
    "        \n",
    "        mlp_user_embedding_wv = generate_embedding(number_of_users, latent_dim_mlp, \"mlp_user_embedding_wv\", reg_mlp)\n",
    "        mlp_item_embedding_wv = generate_embedding(number_of_items, latent_dim_mlp, \"mlp_item_embedding_wv\", reg_mlp)\n",
    "\n",
    "        # word2vec embeddings\n",
    "        product_vector = Flatten()(product_embedding_layer(item))\n",
    "\n",
    "        # Pretrained MF embeddings\n",
    "\n",
    "    #     pretrained_mf_item_latent = Flatten()(pretrained_product_embedding_mf(item))\n",
    "    #     pretrained_mf_user_latent = Flatten()(pretrained_user_embedding_mf(user))\n",
    "    #     mf_cat_latent = Multiply()([pretrained_mf_user_latent, pretrained_mf_item_latent])\n",
    "\n",
    "        # MF vector\n",
    "\n",
    "        mf_user_latent = Flatten()(mf_user_embedding(user))\n",
    "        mf_item_latent = Flatten()(mf_item_embedding(item))\n",
    "        #mf_cat_latent = Dot(axes=1)([mf_user_latent, mf_item_latent])\n",
    "        mf_cat_latent = Multiply()([mf_user_latent, mf_item_latent])\n",
    "        if dropout_on_multiplication_boolean:\n",
    "            mf_cat_latent = Dropout(dropout_on_multiplication)(mf_cat_latent)\n",
    "        # MLP vector 1\n",
    "        mlp_user_latent = Flatten()(mlp_user_embedding(user))\n",
    "        mlp_item_latent = Flatten()(mlp_item_embedding(item))\n",
    "        \n",
    "        # MLP vector 2\n",
    "        mlp_user_latent_wv = Flatten()(mlp_user_embedding_wv(user))\n",
    "        mlp_item_latent_wv = Flatten()(mlp_item_embedding_wv(item))\n",
    "\n",
    "\n",
    "        # MLP vector 1 Concatenation\n",
    "        \n",
    "        mlp_cat_latent = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "\n",
    "        # MLP vector 2 Concatenation\n",
    "\n",
    "        mlp_cat_latent_wv = Concatenate()([mlp_user_latent_wv, product_vector])\n",
    "        \n",
    "#         mlp_user_w2v = Concatenate()([mlp_user_latent, product_vector])\n",
    "#         mlp_item_w2v = Concatenate()([mlp_item_latent, product_vector])\n",
    "#         mlp_3pronged = Concatenate()([mlp_user_latent, product_vector, mlp_item_latent])\n",
    "#         mlp_cat_latent = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "#         mlp_cat_latent = Concatenate()([mlp_user_w2v, mlp_item_w2v])\n",
    "        \n",
    "        if (architecture == 'vanilla_NCF') or (architecture == 'mf_only') or (architecture == 'vanilla_NCF_right'):    \n",
    "            mlp_vector = mlp_cat_latent\n",
    "        elif (architecture == 'w2v_NCF') or (architecture == 'w2v_NCF_right'):\n",
    "            mlp_vector = Concatenate()([mlp_cat_latent, mlp_cat_latent_wv])\n",
    "        \n",
    "\n",
    "        # Start of NN Architecture\n",
    "    \n",
    "        mlp_vector = Dropout(dropout_input)(mlp_vector)\n",
    "        # build dense layers for model\n",
    "        for i in range(len(dense_layers)):\n",
    "            layer = Dense(\n",
    "                dense_layers[i],\n",
    "                activity_regularizer=l2(reg_layers[i]),\n",
    "#                 activation=activation_dense,\n",
    "                activation='linear',\n",
    "                name=\"layer%d\" % i,\n",
    "            )\n",
    "            mlp_vector = layer(mlp_vector)\n",
    "            relu_layer = tf.keras.layers.ReLU()\n",
    "            mlp_vector = relu_layer(mlp_vector)\n",
    "            mlp_vector = Dropout(dropout_hidden_layer)(mlp_vector)\n",
    "            if batch_norm_boolean:\n",
    "                mlp_vector = BatchNormalization()(mlp_vector)\n",
    "\n",
    "    #     # build dense layers for model\n",
    "    #     for i in range(len(dense_layers)):\n",
    "    #         layer = Dense(\n",
    "    #             dense_layers[i],\n",
    "    #             activity_regularizer=l2(reg_layers[i]),\n",
    "    #             activation=activation_dense,\n",
    "    #             name=\"wv_layer%d\" % i,\n",
    "    #         )\n",
    "    #         mlp_vector_wv = layer(mlp_vector_wv)  \n",
    "\n",
    "        if (architecture == 'mf_only'):\n",
    "            predict_layer = mf_cat_latent\n",
    "        elif (architecture == 'vanilla_NCF_right') or (architecture == 'w2v_NCF_right'):\n",
    "            predict_layer = mlp_vector\n",
    "        elif (architecture == 'vanilla_NCF') or (architecture == 'w2v_NCF'):\n",
    "            predict_layer = Concatenate()([mf_cat_latent, mlp_vector])\n",
    "\n",
    "        result = Dense(\n",
    "            1, activation=\"sigmoid\", kernel_initializer=\"lecun_uniform\", name=\"interaction\"\n",
    "        )\n",
    "    \n",
    "        #predict_layer_post_dense = intermediate_result(predict_layer)\n",
    "        output = result(predict_layer)\n",
    "        #output = Flatten()(result(predict_layer))\n",
    "        #output = result(mf_cat_latent)\n",
    "\n",
    "        model = Model(\n",
    "            inputs=[user, item],\n",
    "            outputs=[output],\n",
    "        )\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb53847",
   "metadata": {},
   "source": [
    "# Pipeline Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d7b006ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '32g')\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .config(\"spark.driver.memory\", \"64g\")\\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "sc = SparkContext.getOrCreate() \n",
    "sqlContext = SQLContext(sc)\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c903142",
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = [\"interaction_prior_20k_rank250_100recs\", \"interaction_prior_20k_rank250_100recs_with_hardnegatives_2neighbors\", \"interaction_prior_20k_rank250_100recs_with_hardnegatives_4neighbors\",\n",
    "                 \"interaction_prior_20k_rank250_100recs_with_hardnegatives_6neighbors\", \"interaction_prior_20k_rank250_100recs_with_hardnegatives_8neighbors\",\n",
    "                 \"interaction_prior_20k_rank250_100recs_with_hardnegatives_10neighbors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45115c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations = [(\"interaction_prior_20k_rank250_100recs\", 'vanilla_NCF_right'), (\"interaction_prior_20k_rank250_100recs_with_hardnegatives_4neighbors\", 'vanilla_NCF_right'), \n",
    "#                   (\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF_right'), (\"interaction_prior_20k_rank250_100recs_with_hardnegatives_4neighbors\", 'w2v_NCF_right')]\n",
    "\n",
    "configurations = [(\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF'), (\"interaction_prior_20k_rank250_100recs\", 'vanilla_NCF')]\n",
    "\n",
    "#, (\"interaction_prior_20k_rank250_100recs_with_hard_negatives(1)_withpositives_withrns(5)\", 'w2v_NCF_right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5720c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(configurations)):\n",
    "    #train_filename = \"interaction_prior_20k_rank250_100recs_with_hardnegatives_2neighbors\"\n",
    "    train_filename = configurations[i][0]\n",
    "    architecture = configurations[i][1]\n",
    "    dropout_input = 0.5\n",
    "    dropout_hidden_layer = 0.2\n",
    "    dropout_on_multiplication_boolean = True\n",
    "    dropout_on_multiplication = 0.3\n",
    "    latent_dim_mf  = 256\n",
    "    latent_dim_mlp = 128\n",
    "    reg_mf = 0\n",
    "    reg_mlp = 0.001\n",
    "    dense_layers = [1024, 512, 256]\n",
    "    reg_layers = [0.001, 0.001, 0.001]\n",
    "    activation_dense = \"relu\"\n",
    "    N_EPOCHS = 10\n",
    "    BATCHSIZE = 128\n",
    "    batch_norm_boolean = True\n",
    "\n",
    "    val_filename = \"DLS_Project/interaction_train_20k_rank250_100recs.csv\"\n",
    "    ground_truth_filename = \"DLS_Project/cg_interaction_ground_truth_20k_rank250_100recs.pkl\"\n",
    "    results_filename = \"DLS_Project/Test5_VanillaNCF_w2vNCF_testing_with_mfdropout.txt\"\n",
    "\n",
    "    BATCHSIZE = BATCHSIZE * mirrored_strategy.num_replicas_in_sync\n",
    "\n",
    "    df_train = pd.read_csv(\"DLS_Project/\" + train_filename + \".csv\")\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "                   'balanced',\n",
    "                    np.unique(df_train.interaction.tolist()), \n",
    "                    df_train.interaction.tolist()) \n",
    "\n",
    "    train_user_n, train_product_n = df_train.user_id.nunique(), 49688\n",
    "    # print (df_train.shape)\n",
    "\n",
    "    print (train_filename)\n",
    "\n",
    "    ds_train, ds_val = make_tf_dataset_train(df_train, ['interaction'], batch_size=BATCHSIZE)\n",
    "    del df_train\n",
    "    gc.collect()\n",
    "    df_train = pd.DataFrame()\n",
    "\n",
    "    with mirrored_strategy.scope():\n",
    "\n",
    "        print (\"batchsize:\", BATCHSIZE)\n",
    "        # policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "        # tf.keras.mixed_precision.experimental.set_policy(policy) \n",
    "\n",
    "        ncf_model = create_ncf(architecture, dropout_input, dropout_hidden_layer, dropout_on_multiplication_boolean, dropout_on_multiplication, batch_norm_boolean, train_user_n, train_product_n,\n",
    "                               latent_dim_mf, latent_dim_mlp, reg_mf, reg_mlp, dense_layers, reg_layers)\n",
    "\n",
    "        ncf_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "    #         loss=\"binary_crossentropy\",\n",
    "            loss='binary_crossentropy',\n",
    "            #loss=\"hinge\",\n",
    "            metrics=[\n",
    "                tf.keras.metrics.TruePositives(name=\"tp\"),\n",
    "                tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
    "                tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "                tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "                tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "                tf.keras.metrics.Precision(name=\"precision\"),\n",
    "                tf.keras.metrics.Recall(name=\"recall\"),\n",
    "                tf.keras.metrics.AUC(name=\"auc\")\n",
    "            ],\n",
    "            weighted_metrics=[tf.keras.metrics.BinaryAccuracy(name=\"weighted_accuracy\"),\n",
    "                tf.keras.metrics.Precision(name=\"weighted_precision\"),\n",
    "                tf.keras.metrics.Recall(name=\"weighted_recall\"),\n",
    "                tf.keras.metrics.AUC(name=\"weighted_auc\")]\n",
    "        )\n",
    "        ncf_model._name = \"neural_collaborative_filtering\"\n",
    "        print (ncf_model.summary())\n",
    "        logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, profile_batch=(1,2))\n",
    "        early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_precision\", patience=2, restore_best_weights = True,\n",
    "        )\n",
    "\n",
    "        lr_scheduler_callback = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "        train_hist = ncf_model.fit(\n",
    "            ds_train,\n",
    "            validation_data=ds_val,\n",
    "            epochs=N_EPOCHS,\n",
    "            #callbacks=[tensorboard_callback, early_stopping_callback],\n",
    "            #callbacks=[tensorboard_callback, lr_scheduler_callback, MemoryPrintingCallback()],\n",
    "            #callbacks=[tensorboard_callback],\n",
    "            callbacks=[lr_scheduler_callback],\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            workers=12,\n",
    "            use_multiprocessing=True,\n",
    "            max_queue_size=500,\n",
    "        )\n",
    "\n",
    "    df_val, df_truth = load_val_make_preds(ncf_model, val_filename, ground_truth_filename)\n",
    "\n",
    "    map_15, ndcg_15 = evaluate_and_save_results(df_val, df_truth)\n",
    "\n",
    "    print (map_15, ndcg_15)\n",
    "    \n",
    "    \n",
    "    # Comment if you don't want to write out results\n",
    "\n",
    "    file1 = open(results_filename,\"a\")\n",
    "    L = [str(train_filename) + \"_\" + str(architecture) + \"_\" + str(dropout_input) + \"_\" + str(dropout_hidden_layer) +\n",
    "                                \"_\" + str(dropout_on_multiplication_boolean) + \"_\" + str(dropout_on_multiplication) + \"_\" + str(latent_dim_mlp) + \"_\" + str(latent_dim_mf) +\n",
    "                                \"_\" + str(reg_mf) + \"_\" + str(reg_mlp) + \"_\" + str(dense_layers) + \"_\" + str(reg_layers) + \"_\" + str(N_EPOCHS) + \"_\" + str(BATCHSIZE) + \"_\" + str(batch_norm_boolean) + \"_\" + str(dropout_on_multiplication_boolean) + \n",
    "                                                      \"_\" + str(dropout_on_multiplication) + \"\\n\",\n",
    "        \"MAP 15: \" + str(map_15) + \"\\n\",\n",
    "        \"NDCG 15: \" + str(ndcg_15) + \"\\n\", \"\\n \\n\"]\n",
    "    file1.writelines(L)\n",
    "    file1.close() #to change file access modes\n",
    "    \n",
    "    save_model_artifacts(train_hist, ncf_model, train_filename, architecture, dropout_input, dropout_hidden_layer, dropout_on_multiplication_boolean, dropout_on_multiplication,\n",
    "                            latent_dim_mf, latent_dim_mlp, reg_mf, reg_mlp, dense_layers, reg_layers, N_EPOCHS, BATCHSIZE)\n",
    "\n",
    "    del ncf_model \n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71490a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4eda3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(.2078-0.1798)/(0.1798)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.12299531070321705 0.23477773249508563"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d828925",
   "metadata": {},
   "source": [
    "# Pipeline End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0bc293",
   "metadata": {},
   "source": [
    "# Pipeline 2 Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "40c5b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations = [(\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF', 5),\n",
    "#                   (\"interaction_prior_20k_rank250_100recs_withrns(5)\", 'w2v_NCF', 5),\n",
    "#                   (\"interaction_prior_20k_rank250_100recs_with_hard_negatives(1)_withpositives_withrns(5)\", 'w2v_NCF', 5),\n",
    "#                  (\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF_right', 10),\n",
    "#                   (\"interaction_prior_20k_rank250_100recs_withrns(5)\", 'w2v_NCF_right', 10),\n",
    "#                   (\"interaction_prior_20k_rank250_100recs_with_hard_negatives(1)_withpositives_withrns(5)\", 'w2v_NCF_right', 10),\n",
    "#                  (\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF_right', 15),\n",
    "#                   (\"interaction_prior_20k_rank250_100recs_withrns(5)\", 'w2v_NCF_right', 15),\n",
    "#                   (\"interaction_prior_20k_rank250_100recs_with_hard_negatives(1)_withpositives_withrns(5)\", 'w2v_NCF_right', 15)]\n",
    "\n",
    "\n",
    "configurations = [(\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF', 1, 256),\n",
    "                 (\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF', 2, 256),\n",
    "                  (\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF', 3, 256),\n",
    "                 (\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF', 4, 256)]\n",
    "\n",
    "# configurations = [(\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF', 1, 32),\n",
    "#                   (\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF', 2, 32),\n",
    "#                   (\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF', 3, 32),\n",
    "#                   (\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF', 4, 32),\n",
    "#                   (\"interaction_prior_20k_rank250_100recs\", 'w2v_NCF', 5, 32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8e17b7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction_prior_20k_rank250_100recs\n",
      "batchsize: 256\n",
      "Model: \"neural_collaborative_filtering\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_id (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "product_id (InputLayer)         [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mlp_user_embedding (Embedding)  (None, 180)          3711600     user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mlp_item_embedding (Embedding)  (None, 180)          8943840     product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlp_user_embedding_wv (Embeddin (None, 180)          3711600     user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "w2v_product_vector (Embedding)  (None, 100)          4968800     product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 180)          0           mlp_user_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 180)          0           mlp_item_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 180)          0           mlp_user_embedding_wv[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 100)          0           w2v_product_vector[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 360)          0           flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 280)          0           flatten_12[0][0]                 \n",
      "                                                                 flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 640)          0           concatenate_4[0][0]              \n",
      "                                                                 concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 640)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer0 (Dense)                  (None, 512)          328192      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 512)          0           layer0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 512)          0           re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mf_user_embedding (Embedding)   (None, 256)          5278720     user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mf_item_embedding (Embedding)   (None, 256)          12720128    product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 256)          131328      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 256)          0           mf_user_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 256)          0           mf_item_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 256)          0           layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 256)          0           flatten_8[0][0]                  \n",
      "                                                                 flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256)          0           re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 512)          0           multiply[0][0]                   \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "interaction (Dense)             (None, 1)            513         concatenate_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 39,794,721\n",
      "Trainable params: 34,825,921\n",
      "Non-trainable params: 4,968,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "11326/11326 [==============================] - 769s 67ms/step - loss: 0.6307 - tp: 1320858.0000 - fp: 171748.0000 - tn: 1024631.0000 - fn: 382053.0000 - accuracy: 0.8090 - precision: 0.8849 - recall: 0.7756 - auc: 0.8915 - weighted_accuracy: 0.8160 - weighted_precision: 0.8438 - weighted_recall: 0.7757 - weighted_auc: 0.8914 - val_loss: 0.3530 - val_tp: 362030.0000 - val_fp: 24983.0000 - val_tn: 274131.0000 - val_fn: 63678.0000 - val_accuracy: 0.8777 - val_precision: 0.9354 - val_recall: 0.8504 - val_auc: 0.9387 - val_weighted_accuracy: 0.8834 - val_weighted_precision: 0.9106 - val_weighted_recall: 0.8504 - val_weighted_auc: 0.9387\n",
      "0\n",
      "5000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20357142605012496 0.337229498480553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction_prior_20k_rank250_100recs\n",
      "batchsize: 256\n",
      "Model: \"neural_collaborative_filtering\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_id (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "product_id (InputLayer)         [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mlp_user_embedding (Embedding)  (None, 180)          3711600     user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mlp_item_embedding (Embedding)  (None, 180)          8943840     product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlp_user_embedding_wv (Embeddin (None, 180)          3711600     user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "w2v_product_vector (Embedding)  (None, 100)          4968800     product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 180)          0           mlp_user_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 180)          0           mlp_item_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 180)          0           mlp_user_embedding_wv[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           w2v_product_vector[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 360)          0           flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 280)          0           flatten_5[0][0]                  \n",
      "                                                                 flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 640)          0           concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 640)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer0 (Dense)                  (None, 512)          328192      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 512)          0           layer0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "mf_user_embedding (Embedding)   (None, 256)          5278720     user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mf_item_embedding (Embedding)   (None, 256)          12720128    product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 256)          131328      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 256)          0           mf_user_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 256)          0           mf_item_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 256)          0           layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 256)          0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           multiply[0][0]                   \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "interaction (Dense)             (None, 1)            513         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 39,794,721\n",
      "Trainable params: 34,825,921\n",
      "Non-trainable params: 4,968,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "11326/11326 [==============================] - 767s 67ms/step - loss: 0.6315 - tp: 1321502.0000 - fp: 171484.0000 - tn: 1024895.0000 - fn: 381409.0000 - accuracy: 0.8093 - precision: 0.8851 - recall: 0.7760 - auc: 0.8915 - weighted_accuracy: 0.8163 - weighted_precision: 0.8441 - weighted_recall: 0.7760 - weighted_auc: 0.8915 - val_loss: 0.3540 - val_tp: 362082.0000 - val_fp: 24864.0000 - val_tn: 274250.0000 - val_fn: 63626.0000 - val_accuracy: 0.8779 - val_precision: 0.9357 - val_recall: 0.8505 - val_auc: 0.9386 - val_weighted_accuracy: 0.8837 - val_weighted_precision: 0.9110 - val_weighted_recall: 0.8505 - val_weighted_auc: 0.9386\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "11326/11326 [==============================] - 755s 67ms/step - loss: 0.1135 - tp: 1636024.0000 - fp: 20248.0000 - tn: 1176131.0000 - fn: 66887.0000 - accuracy: 0.9699 - precision: 0.9878 - recall: 0.9607 - auc: 0.9960 - weighted_accuracy: 0.9719 - weighted_precision: 0.9827 - weighted_recall: 0.9607 - weighted_auc: 0.9960 - val_loss: 0.3949 - val_tp: 383808.0000 - val_fp: 45925.0000 - val_tn: 253189.0000 - val_fn: 41900.0000 - val_accuracy: 0.8788 - val_precision: 0.8931 - val_recall: 0.9016 - val_auc: 0.9436 - val_weighted_accuracy: 0.8740 - val_weighted_precision: 0.8545 - val_weighted_recall: 0.9016 - val_weighted_auc: 0.9436\n",
      "0\n",
      "5000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1844476559373292 0.3174300370848664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction_prior_20k_rank250_100recs\n",
      "batchsize: 256\n",
      "Model: \"neural_collaborative_filtering\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_id (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "product_id (InputLayer)         [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mlp_user_embedding (Embedding)  (None, 180)          3711600     user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mlp_item_embedding (Embedding)  (None, 180)          8943840     product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlp_user_embedding_wv (Embeddin (None, 180)          3711600     user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "w2v_product_vector (Embedding)  (None, 100)          4968800     product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 180)          0           mlp_user_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 180)          0           mlp_item_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 180)          0           mlp_user_embedding_wv[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           w2v_product_vector[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 360)          0           flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 280)          0           flatten_5[0][0]                  \n",
      "                                                                 flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 640)          0           concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 640)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer0 (Dense)                  (None, 512)          328192      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 512)          0           layer0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "mf_user_embedding (Embedding)   (None, 256)          5278720     user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mf_item_embedding (Embedding)   (None, 256)          12720128    product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 256)          131328      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 256)          0           mf_user_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 256)          0           mf_item_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 256)          0           layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 256)          0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           multiply[0][0]                   \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "interaction (Dense)             (None, 1)            513         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 39,794,721\n",
      "Trainable params: 34,825,921\n",
      "Non-trainable params: 4,968,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "11326/11326 [==============================] - 769s 67ms/step - loss: 0.6294 - tp: 1321213.0000 - fp: 171257.0000 - tn: 1025122.0000 - fn: 381698.0000 - accuracy: 0.8093 - precision: 0.8853 - recall: 0.7759 - auc: 0.8919 - weighted_accuracy: 0.8163 - weighted_precision: 0.8442 - weighted_recall: 0.7759 - weighted_auc: 0.8919 - val_loss: 0.3537 - val_tp: 363552.0000 - val_fp: 26410.0000 - val_tn: 272704.0000 - val_fn: 62156.0000 - val_accuracy: 0.8778 - val_precision: 0.9323 - val_recall: 0.8540 - val_auc: 0.9388 - val_weighted_accuracy: 0.8829 - val_weighted_precision: 0.9063 - val_weighted_recall: 0.8540 - val_weighted_auc: 0.9388\n",
      "Epoch 2/3\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "11326/11326 [==============================] - 755s 67ms/step - loss: 0.1137 - tp: 1635922.0000 - fp: 20301.0000 - tn: 1176078.0000 - fn: 66989.0000 - accuracy: 0.9699 - precision: 0.9877 - recall: 0.9607 - auc: 0.9959 - weighted_accuracy: 0.9718 - weighted_precision: 0.9826 - weighted_recall: 0.9607 - weighted_auc: 0.9959 - val_loss: 0.3956 - val_tp: 383758.0000 - val_fp: 44970.0000 - val_tn: 254144.0000 - val_fn: 41950.0000 - val_accuracy: 0.8801 - val_precision: 0.8951 - val_recall: 0.9015 - val_auc: 0.9440 - val_weighted_accuracy: 0.8756 - val_weighted_precision: 0.8570 - val_weighted_recall: 0.9015 - val_weighted_auc: 0.9440\n",
      "Epoch 3/3\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0001.\n",
      "11326/11326 [==============================] - 753s 66ms/step - loss: 0.0073 - tp: 1700732.0000 - fp: 374.0000 - tn: 1196005.0000 - fn: 2179.0000 - accuracy: 0.9991 - precision: 0.9998 - recall: 0.9987 - auc: 1.0000 - weighted_accuracy: 0.9992 - weighted_precision: 0.9997 - weighted_recall: 0.9987 - weighted_auc: 1.0000 - val_loss: 0.3949 - val_tp: 383374.0000 - val_fp: 40976.0000 - val_tn: 258138.0000 - val_fn: 42334.0000 - val_accuracy: 0.8851 - val_precision: 0.9034 - val_recall: 0.9006 - val_auc: 0.9446 - val_weighted_accuracy: 0.8818 - val_weighted_precision: 0.8680 - val_weighted_recall: 0.9006 - val_weighted_auc: 0.9446\n",
      "0\n",
      "5000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1881840104657408 0.3220408590430409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction_prior_20k_rank250_100recs\n",
      "batchsize: 256\n",
      "Model: \"neural_collaborative_filtering\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_id (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "product_id (InputLayer)         [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mlp_user_embedding (Embedding)  (None, 180)          3711600     user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mlp_item_embedding (Embedding)  (None, 180)          8943840     product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlp_user_embedding_wv (Embeddin (None, 180)          3711600     user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "w2v_product_vector (Embedding)  (None, 100)          4968800     product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 180)          0           mlp_user_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 180)          0           mlp_item_embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 180)          0           mlp_user_embedding_wv[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 100)          0           w2v_product_vector[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 360)          0           flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 280)          0           flatten_5[0][0]                  \n",
      "                                                                 flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 640)          0           concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 640)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer0 (Dense)                  (None, 512)          328192      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 512)          0           layer0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "mf_user_embedding (Embedding)   (None, 256)          5278720     user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mf_item_embedding (Embedding)   (None, 256)          12720128    product_id[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer1 (Dense)                  (None, 256)          131328      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 256)          0           mf_user_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 256)          0           mf_item_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 256)          0           layer1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 256)          0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           multiply[0][0]                   \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "interaction (Dense)             (None, 1)            513         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 39,794,721\n",
      "Trainable params: 34,825,921\n",
      "Non-trainable params: 4,968,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/4\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "11326/11326 [==============================] - 767s 67ms/step - loss: 0.6311 - tp: 1320192.0000 - fp: 170935.0000 - tn: 1025444.0000 - fn: 382719.0000 - accuracy: 0.8090 - precision: 0.8854 - recall: 0.7753 - auc: 0.8915 - weighted_accuracy: 0.8162 - weighted_precision: 0.8444 - weighted_recall: 0.7753 - weighted_auc: 0.8915 - val_loss: 0.3562 - val_tp: 362690.0000 - val_fp: 25598.0000 - val_tn: 273516.0000 - val_fn: 63018.0000 - val_accuracy: 0.8777 - val_precision: 0.9341 - val_recall: 0.8520 - val_auc: 0.9389 - val_weighted_accuracy: 0.8832 - val_weighted_precision: 0.9087 - val_weighted_recall: 0.8520 - val_weighted_auc: 0.9389\n",
      "Epoch 2/4\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "11326/11326 [==============================] - 757s 67ms/step - loss: 0.1135 - tp: 1636131.0000 - fp: 20318.0000 - tn: 1176061.0000 - fn: 66780.0000 - accuracy: 0.9700 - precision: 0.9877 - recall: 0.9608 - auc: 0.9960 - weighted_accuracy: 0.9719 - weighted_precision: 0.9826 - weighted_recall: 0.9608 - weighted_auc: 0.9960 - val_loss: 0.3935 - val_tp: 383709.0000 - val_fp: 45420.0000 - val_tn: 253694.0000 - val_fn: 41999.0000 - val_accuracy: 0.8794 - val_precision: 0.8942 - val_recall: 0.9013 - val_auc: 0.9439 - val_weighted_accuracy: 0.8747 - val_weighted_precision: 0.8558 - val_weighted_recall: 0.9013 - val_weighted_auc: 0.9439\n",
      "Epoch 3/4\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0001.\n",
      "11326/11326 [==============================] - 756s 67ms/step - loss: 0.0072 - tp: 1700745.0000 - fp: 388.0000 - tn: 1195991.0000 - fn: 2166.0000 - accuracy: 0.9991 - precision: 0.9998 - recall: 0.9987 - auc: 1.0000 - weighted_accuracy: 0.9992 - weighted_precision: 0.9997 - weighted_recall: 0.9987 - weighted_auc: 1.0000 - val_loss: 0.3904 - val_tp: 382604.0000 - val_fp: 38695.0000 - val_tn: 260419.0000 - val_fn: 43104.0000 - val_accuracy: 0.8871 - val_precision: 0.9082 - val_recall: 0.8987 - val_auc: 0.9448 - val_weighted_accuracy: 0.8847 - val_weighted_precision: 0.8742 - val_weighted_recall: 0.8987 - val_weighted_auc: 0.9448\n",
      "Epoch 4/4\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0001.\n",
      "11326/11326 [==============================] - 756s 67ms/step - loss: 0.0036 - tp: 1701793.0000 - fp: 167.0000 - tn: 1196212.0000 - fn: 1118.0000 - accuracy: 0.9996 - precision: 0.9999 - recall: 0.9993 - auc: 1.0000 - weighted_accuracy: 0.9996 - weighted_precision: 0.9999 - weighted_recall: 0.9993 - weighted_auc: 1.0000 - val_loss: 0.4288 - val_tp: 383485.0000 - val_fp: 40773.0000 - val_tn: 258341.0000 - val_fn: 42223.0000 - val_accuracy: 0.8855 - val_precision: 0.9039 - val_recall: 0.9008 - val_auc: 0.9435 - val_weighted_accuracy: 0.8823 - val_weighted_precision: 0.8686 - val_weighted_recall: 0.9008 - val_weighted_auc: 0.9435\n",
      "0\n",
      "5000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1897171915647746 0.32364431276545963\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(configurations)):\n",
    "    #train_filename = \"interaction_prior_20k_rank250_100recs_with_hardnegatives_2neighbors\"\n",
    "    train_filename = configurations[i][0]\n",
    "    architecture = configurations[i][1]\n",
    "    dropout_input = 0.5\n",
    "    dropout_hidden_layer = 0.2\n",
    "    dropout_on_multiplication_boolean = False\n",
    "    dropout_on_multiplication = 0.7\n",
    "    latent_dim_mf  = configurations[i][3]\n",
    "    latent_dim_mlp = 180\n",
    "    reg_mf = 0\n",
    "    reg_mlp = 0.001\n",
    "    dense_layers = [512, 256]\n",
    "    reg_layers = [0.001, 0.001]\n",
    "    activation_dense = \"relu\"\n",
    "    N_EPOCHS = configurations[i][2]\n",
    "    BATCHSIZE = 128\n",
    "    batch_norm_boolean = False\n",
    "\n",
    "    val_filename = \"DLS_Project/interaction_train_20k_rank250_100recs.csv\"\n",
    "    ground_truth_filename = \"DLS_Project/cg_interaction_ground_truth_20k_rank250_100recs.pkl\"\n",
    "    results_filename = \"DLS_Project/Test20_w2vNCF_mfdim256_with_dot.txt\"\n",
    "\n",
    "    BATCHSIZE = BATCHSIZE * mirrored_strategy.num_replicas_in_sync\n",
    "\n",
    "    df_train = pd.read_csv(\"DLS_Project/\" + train_filename + \".csv\")\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight(\n",
    "                   'balanced',\n",
    "                    np.unique(df_train.interaction.tolist()), \n",
    "                    df_train.interaction.tolist()) \n",
    "\n",
    "    train_user_n, train_product_n = df_train.user_id.nunique(), 49688\n",
    "    # print (df_train.shape)\n",
    "\n",
    "    print (train_filename)\n",
    "\n",
    "    ds_train, ds_val = make_tf_dataset_train(df_train, ['interaction'], batch_size=BATCHSIZE)\n",
    "    del df_train\n",
    "    gc.collect()\n",
    "    df_train = pd.DataFrame()\n",
    "\n",
    "    with mirrored_strategy.scope():\n",
    "\n",
    "        print (\"batchsize:\", BATCHSIZE)\n",
    "        # policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "        # tf.keras.mixed_precision.experimental.set_policy(policy) \n",
    "\n",
    "        ncf_model = create_ncf(architecture, dropout_input, dropout_hidden_layer, dropout_on_multiplication_boolean, dropout_on_multiplication, batch_norm_boolean, train_user_n, train_product_n,\n",
    "                               latent_dim_mf, latent_dim_mlp, reg_mf, reg_mlp, dense_layers, reg_layers)\n",
    "\n",
    "        ncf_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "    #         loss=\"binary_crossentropy\",\n",
    "            loss='binary_crossentropy',\n",
    "            #loss=\"hinge\",\n",
    "            metrics=[\n",
    "                tf.keras.metrics.TruePositives(name=\"tp\"),\n",
    "                tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
    "                tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "                tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "                tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "                tf.keras.metrics.Precision(name=\"precision\"),\n",
    "                tf.keras.metrics.Recall(name=\"recall\"),\n",
    "                tf.keras.metrics.AUC(name=\"auc\")\n",
    "            ],\n",
    "            weighted_metrics=[tf.keras.metrics.BinaryAccuracy(name=\"weighted_accuracy\"),\n",
    "                tf.keras.metrics.Precision(name=\"weighted_precision\"),\n",
    "                tf.keras.metrics.Recall(name=\"weighted_recall\"),\n",
    "                tf.keras.metrics.AUC(name=\"weighted_auc\")]\n",
    "        )\n",
    "        ncf_model._name = \"neural_collaborative_filtering\"\n",
    "        print (ncf_model.summary())\n",
    "        logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, profile_batch=(1,2))\n",
    "        early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_precision\", patience=2, restore_best_weights = True,\n",
    "        )\n",
    "\n",
    "        lr_scheduler_callback = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "        train_hist = ncf_model.fit(\n",
    "            ds_train,\n",
    "            validation_data=ds_val,\n",
    "            epochs=N_EPOCHS,\n",
    "            #callbacks=[tensorboard_callback, early_stopping_callback],\n",
    "            #callbacks=[tensorboard_callback, lr_scheduler_callback, MemoryPrintingCallback()],\n",
    "            #callbacks=[tensorboard_callback],\n",
    "            callbacks=[lr_scheduler_callback],\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            workers=12,\n",
    "            use_multiprocessing=True,\n",
    "            max_queue_size=500,\n",
    "        )\n",
    "\n",
    "    df_val, df_truth = load_val_make_preds(ncf_model, val_filename, ground_truth_filename)\n",
    "\n",
    "    map_15, ndcg_15 = evaluate_and_save_results(df_val, df_truth)\n",
    "\n",
    "    print (map_15, ndcg_15)\n",
    "    \n",
    "    \n",
    "    # Comment if you don't want to write out results\n",
    "\n",
    "    file1 = open(results_filename,\"a\")\n",
    "    L = [str(train_filename) + \"_\" + str(architecture) + \"_\" + str(dropout_input) + \"_\" + str(dropout_hidden_layer) +\n",
    "                                \"_\" + str(dropout_on_multiplication_boolean) + \"_\" + str(dropout_on_multiplication) + \"_\" + str(latent_dim_mlp) + \"_\" + str(latent_dim_mf) +\n",
    "                                \"_\" + str(reg_mf) + \"_\" + str(reg_mlp) + \"_\" + str(dense_layers) + \"_\" + str(reg_layers) + \"_\" + str(N_EPOCHS) + \"_\" + str(BATCHSIZE) + \"_\" + str(batch_norm_boolean) + \"_\" + str(dropout_on_multiplication_boolean) + \n",
    "                                                      \"_\" + str(dropout_on_multiplication) + \"\\n\",\n",
    "        \"MAP 15: \" + str(map_15) + \"\\n\",\n",
    "        \"NDCG 15: \" + str(ndcg_15) + \"\\n\", \"\\n \\n\"]\n",
    "    file1.writelines(L)\n",
    "    file1.close() #to change file access modes\n",
    "    \n",
    "    save_model_artifacts(train_hist, ncf_model, train_filename, architecture, dropout_input, dropout_hidden_layer, dropout_on_multiplication_boolean, dropout_on_multiplication,\n",
    "                            latent_dim_mf, latent_dim_mlp, reg_mf, reg_mlp, dense_layers, reg_layers, N_EPOCHS, BATCHSIZE)\n",
    "\n",
    "    del ncf_model \n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd21cea9",
   "metadata": {},
   "source": [
    "# Pipeline 2 End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73225c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_hist.history['loss']\n",
    "\n",
    "# train_hist.history['weighted_precision']\n",
    "\n",
    "# train_hist.history['val_weighted_precision']\n",
    "\n",
    "# type(train_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8211906",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "\n",
    "1. Test out if more epochs produces better results given 6 layers on MLP side\n",
    "2. Test to see if higher dropout rate affects more epochs with 6 layers\n",
    "3. Test to see if more mlp regularization affects more epochs with 6 layers\n",
    "4. Test to see if smaller step size affects more epochs with 6 layers\n",
    "5. Test to see if best hyperparameter set also performs well on the rank 425 set as well as the 300 rec, 100 rec, 150 rec sets\n",
    "6. Test to see how a lower batch size affects performance on MAP@15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5d494",
   "metadata": {},
   "source": [
    "### Saving and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1a2b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving Model\n",
    "\n",
    "pd.DataFrame.from_dict(\n",
    "    train_hist.history).to_csv('DLS_Project/train_history_10epochs_withhn.csv')\n",
    "\n",
    "ncf_model.save('DLS_Project/10epochs_nohardnegatives_batchsize32_withhn')\n",
    "\n",
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"DLS_Project/train_history_10epochs_withhn.csv\")\n",
    "\n",
    "pd.read_csv(\"DLS_Project/train_history_10epochs_withhn.csv\")['val_weighted_precision']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ec169",
   "metadata": {},
   "source": [
    "### Loading Model and Validation Set & Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ncf_model = keras.models.load_model('CG_NCF_40k_Test5_9Layers_VanillaNCF_256/')\n",
    "\n",
    "### Load Validation Set\n",
    "\n",
    "df_val = pd.read_csv(\"DLS_Project/interaction_train_20k_rank250_100recs.csv\")\n",
    "df_truth = pd.read_pickle(\"DLS_Project/cg_interaction_ground_truth_20k_rank250_100recs.pkl\")\n",
    "\n",
    "# all_products = products.product_id.unique()\n",
    "# all_products.sort()\n",
    "# product_zerobased_map = dict(zip(all_products, range(len(all_products))))\n",
    "# df_val.product_id = df_val.product_id.map(product_zerobased_map)\n",
    "\n",
    "ds_test, _ = make_tf_dataset_val(df_val, ['interaction'], val_split=0, seed=None)\n",
    "print ('done making batch dataset')\n",
    "\n",
    "#%%time\n",
    "tic = time.perf_counter()\n",
    "ncf_predictions = ncf_model.predict(ds_test, batch_size=512, max_queue_size=500, verbose=0)\n",
    "toc = time.perf_counter()\n",
    "print ('done making predictions')\n",
    "print (f\"in {toc - tic:0.4f}\")\n",
    "\n",
    "df_val[\"ncf_predictions\"] = ncf_predictions\n",
    "df_val[\"pred\"] = (df_val.ncf_predictions >= 0.5) * 1.0\n",
    "df_val[\"correct\"] = (df_val.interaction == df_val.pred) * 1.0\n",
    "df_val.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b77bab9",
   "metadata": {},
   "source": [
    "### Eye Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e9f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_study = 5388"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2396b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_truth[['user_id']].drop_duplicates().sort_values('user_id').iloc[0:20]\n",
    "# df_val[['user_id']].drop_duplicates().sort_values('user_id').iloc[0:20]\n",
    "products = pd.read_csv(\"products.csv\")\n",
    "products.product_id = products.product_id - 1\n",
    "df_val_complete = pd.merge(df_val, products[['product_id', 'product_name']], how='left', on='product_id')\n",
    "df_val_complete[(df_val_complete.user_id == user_study)].sort_values(\"ncf_predictions\", ascending=False).iloc[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_complete[(df_val_complete.user_id == user_study) & (df_val_complete.interaction == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd9fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_truth = pd.merge(df_truth, products[['product_id', 'product_name']], how='left', on='product_id')\n",
    "# df_truth[df_truth.user_id == 4550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04aaf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_truth.to_csv(\"allusers_rank350_groundtruth.csv\")\n",
    "\n",
    "# df_val_complete[(df_val_complete.user_id == 22140)]\n",
    "\n",
    "# df_val_complete.to_csv(\"allusers_rank350_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e8c9f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '32g')\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .config(\"spark.driver.memory\", \"64g\")\\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "sc = SparkContext.getOrCreate() \n",
    "sqlContext = SQLContext(sc)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "std = df_val.ncf_predictions.std()\n",
    "if std < 0.01:\n",
    "    raise ValueError(\"Model predictions have standard deviation of less than 1e-2.\")\n",
    "\n",
    "all_indices_predictions = df_val.user_id.unique()\n",
    "all_indices_predictions.sort()\n",
    "\n",
    "sorted_df_val = df_val[['user_id', 'product_id', 'ncf_predictions']].sort_values(by=['user_id', 'ncf_predictions'], ascending=[True, False])\n",
    "gb_sorted_df_val = sorted_df_val.groupby('user_id')\n",
    "gb_df_truth = df_truth.groupby('user_id')\n",
    "\n",
    "predictionAndLabelsAll = []\n",
    "counter = 0\n",
    "for i in all_indices_predictions:\n",
    "    if counter % 5000 == 0:\n",
    "        print (counter)\n",
    "    counter+=1 \n",
    "    preds = gb_sorted_df_val.get_group(i).product_id.tolist()\n",
    "    truth = gb_df_truth.get_group(i).product_id.tolist()\n",
    "    predictionAndLabelsAll.append((preds, truth))\n",
    "\n",
    "predictionAndLabels_All = sc.parallelize(predictionAndLabelsAll)\n",
    "metrics_all = RankingMetrics(predictionAndLabels_All)\n",
    "print (metrics_all.meanAveragePrecisionAt(15))\n",
    "print (metrics_all.ndcgAt(15))\n",
    "print (metrics_all.precisionAt(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9534905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.0633\n",
    "# 0.1486\n",
    "\n",
    "(.2078-0.1798)/(0.1798)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics_all.meanAveragePrecisionAt(100))\n",
    "print (metrics_all.ndcgAt(100))\n",
    "print (metrics_all.precisionAt(100))\n",
    "print (metrics_all.recallAt(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346623ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch --> .2132, .3471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truth.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b530c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6899ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_list = all_indices_predictions\n",
    "\n",
    "df_results = pd.DataFrame(user_id_list, columns=['user_id'])\n",
    "\n",
    "file_name = 'user_study_dict_ALLUSERS.pkl'\n",
    "\n",
    "open_file = open(file_name, \"rb\")\n",
    "dict_user_id = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "reverse_user_map = {v: k for k, v in dict_user_id.items()}\n",
    "\n",
    "df_results[\"MAP_15\"] = None\n",
    "df_results[\"NDCG_15\"] = None\n",
    "\n",
    "df_results[\"true_user_id\"] = df_results.user_id.map(reverse_user_map)\n",
    "\n",
    "for i in range(len(predictionAndLabelsAll)):\n",
    "    if i%10==0:\n",
    "        print (i)\n",
    "    prediction_and_labels = sc.parallelize(predictionAndLabelsAll[i:i+1])\n",
    "    metrics_all = RankingMetrics(prediction_and_labels)\n",
    "    df_results.loc[i, \"MAP_15\"] = metrics_all.meanAveragePrecisionAt(15)\n",
    "    df_results.loc[i, \"NDCG_15\"] = metrics_all.ndcgAt(15)\n",
    "    \n",
    "df_results.to_pickle(\"df_results_ALLUSERS_v1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b1f14d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f3160",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97aa011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val = pd.merge(df_val, products, how='left', on='product_id')\n",
    "\n",
    "# df_val.head()\n",
    "\n",
    "\n",
    "\n",
    "# df_train = pd.merge(df_train, products, how='left', on='product_id')\n",
    "\n",
    "# df_val.head()\n",
    "\n",
    "### Past Purchases\n",
    "\n",
    "# df_train[(df_train.user_id == 1557) & (df_train.interaction == 1)].product_name.value_counts().iloc[0:15]\n",
    "\n",
    "# df_train[(df_train.user_id == 1557) & (df_train.interaction == 0)].product_name.value_counts().iloc[0:15]\n",
    "\n",
    "# df_val[(df_val.user_id == 1557) & (df_val.interaction == 0)].product_name.value_counts().iloc[0:15]\n",
    "\n",
    "### Actual Purchases\n",
    "\n",
    "# df_val[(df_val.user_id == 1557) & (df_val.interaction == 1)].product_name.value_counts().iloc[0:15]\n",
    "\n",
    "# df_val.user_id.unique()[0:10]\n",
    "\n",
    "# df_val[(df_val.user_id == 2402)].sort_values(\"ncf_predictions\", ascending=False).iloc[0:15]\n",
    "\n",
    "# df_val[(df_val.user_id == 2402) & (df_val.interaction == 1)].sort_values(\"ncf_predictions\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5022e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# std = df_val.ncf_predictions.std()\n",
    "# if std < 0.01:\n",
    "#     raise ValueError(\"Model predictions have standard deviation of less than 1e-2.\")\n",
    "\n",
    "\n",
    "# df_ncf_predictions = pd.DataFrame()\n",
    "# df_test = pd.DataFrame()\n",
    "\n",
    "\n",
    "# def split(a, n):\n",
    "#     k, m = divmod(len(a), n)\n",
    "#     return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "# counter = 0\n",
    "# for index_chunk in list(split(df_val.user_id.unique(), 10)):\n",
    "#     counter += 1\n",
    "#     if counter%2==0:\n",
    "#         print (counter)\n",
    "#     df_ncf_predictions = df_ncf_predictions.append(df_val[df_val.user_id.isin(index_chunk)].pivot(\n",
    "#     index=\"user_id\", columns=\"product_id\", values=\"ncf_predictions\"))\n",
    "    \n",
    "#     df_test = df_test.append(df_truth[df_truth.user_id.isin(index_chunk)].pivot(\n",
    "#         index=\"user_id\", columns=\"product_id\", values=\"interaction\"))\n",
    "    \n",
    "    \n",
    "# #     df_test = df_test.append(df_val[df_val.user_id.isin(index_chunk)].pivot(\n",
    "# #     index=\"user_id\", columns=\"product_id\", values=\"interaction\"))\n",
    "\n",
    "# data = {}\n",
    "\n",
    "# data[\"ncf_predictions\"] = df_ncf_predictions.values\n",
    "# data[\"test\"] = df_test.values\n",
    "\n",
    "# # data[\"test\"] = df_test.values\n",
    "\n",
    "# # data = {}\n",
    "\n",
    "# # data[\"ncf_predictions\"] = df_val.pivot(\n",
    "# #     index=\"user_id\", columns=\"product_id\", values=\"ncf_predictions\"\n",
    "# # ).values\n",
    "\n",
    "# # data[\"test\"] = df_val.pivot(\n",
    "# #     index=\"user_id\", columns=\"product_id\", values=\"interaction\"\n",
    "# # ).values\n",
    "\n",
    "# print ('done')\n",
    "\n",
    "\n",
    "# # capturing all the users in df_ncf_predictions and sorting them\n",
    "\n",
    "# all_indices_predictions = df_ncf_predictions.index.tolist()\n",
    "\n",
    "# all_indices_predictions.sort()\n",
    "# all_indices_predictions\n",
    "# print (len(all_indices_predictions))\n",
    "\n",
    "# # capturing all the products in df_ncf_predictions and df_test\n",
    "\n",
    "# all_products_predictions = df_ncf_predictions.columns.to_list()\n",
    "# all_products_results = df_test.columns.to_list()\n",
    "\n",
    "# prediction_dict = dict(zip(range(len(all_products_predictions)), all_products_predictions))\n",
    "# result_dict = dict(zip(range(len(all_products_results)), all_products_results))\n",
    "\n",
    "# SparkContext.setSystemProperty('spark.executor.memory', '32g')\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local[1]\") \\\n",
    "#     .appName(\"SparkByExamples.com\") \\\n",
    "#     .config(\"spark.driver.memory\", \"64g\")\\\n",
    "#     .getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "# sc = SparkContext.getOrCreate() \n",
    "# sqlContext = SQLContext(sc)\n",
    "# sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "# predictionAndLabelsAll = []\n",
    "# counter = 0\n",
    "# for i in all_indices_predictions:\n",
    "#     if counter % 10 == 0:\n",
    "#         print (counter)\n",
    "#     counter+=1\n",
    "#     ncf_predictions_tuple = list(enumerate(df_ncf_predictions.loc[i]))\n",
    "#     ncf_predictions_tuple = [x for x in list(enumerate(df_ncf_predictions.loc[i])) if not np.isnan(x[1])]\n",
    "#     preds_complete = [x for x in Sort_Tuple(ncf_predictions_tuple)[0:100]]\n",
    "#     preds = [x[0] for x in Sort_Tuple(ncf_predictions_tuple)[0:100]]\n",
    "# #     print (preds_complete)\n",
    "# #     print ('--------------------------------------------------------------------------------------------------------------')\n",
    "#     preds = [prediction_dict[x] for x in preds]\n",
    "# #     print (preds)\n",
    "# #     print ('--------------------------------------------------------------------------------------------------------------')\n",
    "#     test_tuple = list(enumerate(df_test.loc[i]))\n",
    "#     test_tuple = [x for x in list(enumerate(df_test.loc[i])) if not np.isnan(x[1])]\n",
    "#     truth = [x[0] for x in Sort_Tuple(test_tuple) if x[1] == 1]\n",
    "#     truth = [result_dict[x] for x in truth]\n",
    "#     predictionAndLabelsAll.append((preds, truth))\n",
    "# #     print (truth)\n",
    "#     break\n",
    "# predictionAndLabels_All = sc.parallelize(predictionAndLabelsAll)\n",
    "# metrics_all = RankingMetrics(predictionAndLabels_All)\n",
    "# print (metrics_all.meanAveragePrecisionAt(15))\n",
    "# print (metrics_all.ndcgAt(15))\n",
    "# print (metrics_all.precisionAt(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821f042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce2db0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba222e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2added4b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab35bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748164a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b28f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_list = all_indices_predictions\n",
    "\n",
    "df_results = pd.DataFrame(user_id_list, columns=['user_id'])\n",
    "\n",
    "file_name = 'user_zerobased_map_for_als_comparison.pkl'\n",
    "\n",
    "open_file = open(file_name, \"rb\")\n",
    "dict_user_id = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "reverse_user_map = {v: k for k, v in dict_user_id.items()}\n",
    "\n",
    "df_results[\"MAP_15\"] = None\n",
    "df_results[\"NDCG_15\"] = None\n",
    "\n",
    "df_results[\"true_user_id\"] = df_results.user_id.map(reverse_user_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3678e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results.user_id == 1557]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0490312",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_history = pd.read_pickle('user_history.pkl')\n",
    "\n",
    "#df_results = df_results.drop('user_id', axis=1)\n",
    "\n",
    "df_results = pd.merge(df_results, df_user_history, how='left', left_on='true_user_id', right_on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results.sort_values(\"MAP_15\", ascending=False).iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ddfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val[df_val.user_id == 68142]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_indices_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef8d54e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(predictionAndLabelsAll)):\n",
    "    if i%10==0:\n",
    "        print (i)\n",
    "    prediction_and_labels = sc.parallelize(predictionAndLabelsAll[i:i+1])\n",
    "    metrics_all = RankingMetrics(prediction_and_labels)\n",
    "    df_results.loc[i, \"MAP_15\"] = metrics_all.meanAveragePrecisionAt(15)\n",
    "    df_results.loc[i, \"NDCG_15\"] = metrics_all.ndcgAt(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_pickle(\"df_results_Jan6.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c0644b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc049c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a382660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(data['test'].shape[0]))\n",
    "split_n = 4\n",
    "\n",
    "# precision_scores_all = []\n",
    "# recall_scores_all = []\n",
    "predictionAndLabelsAll = []\n",
    "\n",
    "for index_chunk in list(split(indices, split_n)):\n",
    "    predictionAndLabels = []\n",
    "\n",
    "    counter = 0\n",
    "    precision_scores = []\n",
    "    for i in index_chunk:\n",
    "        print (i)\n",
    "        if i % 10 == 0:\n",
    "            print (i)\n",
    "        \n",
    "#         m_ndcg = tf.keras.metrics.NDCGMetric(topn=15)\n",
    "#         m_ndcg.update_state(data[\"test\"][i], data[\"ncf_predictions\"][i])\n",
    "#         ndcg_scores_all.append(m_ndcg.result().numpy())\n",
    "#         m_ndcg.reset_states()\n",
    "        \n",
    "#         print (counter)\n",
    "#         counter += 1\n",
    "        ncf_predictions_tuple = list(enumerate(data['ncf_predictions'][i]))\n",
    "        ncf_predictions_tuple = [x for x in list(enumerate(data['ncf_predictions'][i])) if not np.isnan(x[1])]\n",
    "#         print (list(enumerate(data['ncf_predictions'][i])))\n",
    "        #test_tuple = list(enumerate(data['test'][i]))\n",
    "        #test_tuple = [x for x in list(enumerate(data['test'][i])) if not np.isnan(x[1])]\n",
    "        preds = [x[0] for x in Sort_Tuple(ncf_predictions_tuple)[0:100]]\n",
    "        #truth = [x[0] for x in Sort_Tuple(test_tuple) if x[1] == 1]\n",
    "        print (preds)\n",
    "        print (truth)\n",
    "        break\n",
    "#         predictionAndLabels.append((preds, truth))\n",
    "        predictionAndLabelsAll.append((preds, truth))\n",
    "#         predictionAndLabels_ind.append((preds, truth))\n",
    "#         metrics = RankingMetrics(predictionAndLabels_ind)\n",
    "#         print (metrics.meanAveragePrecisionAt(15))\n",
    "#     predictionAndLabels = sc.parallelize(predictionAndLabels)\n",
    "#     metrics = RankingMetrics(predictionAndLabels)\n",
    "#     print (metrics.precisionAt(15))\n",
    "#     print (metrics.meanAveragePrecisionAt(15))\n",
    "    break\n",
    "predictionAndLabels_All = sc.parallelize(predictionAndLabelsAll)\n",
    "metrics_all = RankingMetrics(predictionAndLabels_All)\n",
    "print (metrics_all.precisionAt(15))\n",
    "print (metrics_all.meanAveragePrecisionAt(15))\n",
    "print (metrics_all.ndcgAt(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3318f08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6e77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val[(df_val.user_id == 10301) & (df_val.product_id == 49234)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecff216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.distplot(df_val.ncf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = df_val.interaction.tolist()\n",
    "# prob = df_val.ncf_predictions.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if model is well calibrated \n",
    "\n",
    "# from sklearn.calibration import calibration_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x, y = calibration_curve(y_test, prob, n_bins = 10, normalize = True)\n",
    "\n",
    "# # Plot perfectly calibrated\n",
    "# plt.plot([0, 1], [0, 1], linestyle = '--', label = 'Ideally Calibrated')\n",
    " \n",
    "# # Plot model's calibration curve\n",
    "# plt.plot(y, x, marker = '.', label = 'Support Vector Classifier')\n",
    " \n",
    "# leg = plt.legend(loc = 'upper left')\n",
    "# plt.xlabel('Average Predicted Probability in each bin')\n",
    "# plt.ylabel('Ratio of positives')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51b839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_val\n",
    "gc.collect()\n",
    "df_val = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ebef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds_test\n",
    "ds_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "del _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc47206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model_memory_usage(batch_size, model):\n",
    "#     import numpy as np\n",
    "#     try:\n",
    "#         from keras import backend as K\n",
    "#     except:\n",
    "#         from tensorflow.keras import backend as K\n",
    "\n",
    "#     shapes_mem_count = 0\n",
    "#     internal_model_mem_count = 0\n",
    "#     for l in model.layers:\n",
    "#         layer_type = l.__class__.__name__\n",
    "#         if layer_type == 'Model':\n",
    "#             internal_model_mem_count += get_model_memory_usage(batch_size, l)\n",
    "#         single_layer_mem = 1\n",
    "#         out_shape = l.output_shape\n",
    "#         if type(out_shape) is list:\n",
    "#             out_shape = out_shape[0]\n",
    "#         for s in out_shape:\n",
    "#             if s is None:\n",
    "#                 continue\n",
    "#             single_layer_mem *= s\n",
    "#         shapes_mem_count += single_layer_mem\n",
    "\n",
    "#     trainable_count = np.sum([K.count_params(p) for p in model.trainable_weights])\n",
    "#     non_trainable_count = np.sum([K.count_params(p) for p in model.non_trainable_weights])\n",
    "\n",
    "#     number_size = 4.0\n",
    "#     if K.floatx() == 'float16':\n",
    "#         number_size = 2.0\n",
    "#     if K.floatx() == 'float64':\n",
    "#         number_size = 8.0\n",
    "\n",
    "#     total_memory = number_size * (batch_size * shapes_mem_count + trainable_count + non_trainable_count)\n",
    "#     gbytes = np.round(total_memory / (1024.0 ** 3), 3) + internal_model_mem_count\n",
    "#     return gbytes\n",
    "\n",
    "# def keras_model_memory_usage_in_bytes(model, *, batch_size: int):\n",
    "#     \"\"\"\n",
    "#     Return the estimated memory usage of a given Keras model in bytes.\n",
    "#     This includes the model weights and layers, but excludes the dataset.\n",
    "\n",
    "#     The model shapes are multipled by the batch size, but the weights are not.\n",
    "\n",
    "#     Args:\n",
    "#         model: A Keras model.\n",
    "#         batch_size: The batch size you intend to run the model with. If you\n",
    "#             have already specified the batch size in the model itself, then\n",
    "#             pass `1` as the argument here.\n",
    "#     Returns:\n",
    "#         An estimate of the Keras model's memory usage in bytes.\n",
    "\n",
    "#     \"\"\"\n",
    "#     default_dtype = tf.keras.backend.floatx()\n",
    "#     shapes_mem_count = 0\n",
    "#     internal_model_mem_count = 0\n",
    "#     for layer in model.layers:\n",
    "#         if isinstance(layer, tf.keras.Model):\n",
    "#             internal_model_mem_count += keras_model_memory_usage_in_bytes(\n",
    "#                 layer, batch_size=batch_size\n",
    "#             )\n",
    "#         single_layer_mem = tf.as_dtype(layer.dtype or default_dtype).size\n",
    "#         out_shape = layer.output_shape\n",
    "#         if isinstance(out_shape, list):\n",
    "#             out_shape = out_shape[0]\n",
    "#         for s in out_shape:\n",
    "#             if s is None:\n",
    "#                 continue\n",
    "#             single_layer_mem *= s\n",
    "#         shapes_mem_count += single_layer_mem\n",
    "\n",
    "#     trainable_count = sum(\n",
    "#         [tf.keras.backend.count_params(p) for p in model.trainable_weights]\n",
    "#     )\n",
    "\n",
    "#     non_trainable_count = sum(\n",
    "#         [tf.keras.backend.count_params(p) for p in model.non_trainable_weights]\n",
    "#     )\n",
    "#     print (non_trainable_count + trainable_count)\n",
    "#     total_memory = (\n",
    "#         batch_size * shapes_mem_count\n",
    "#         + internal_model_mem_count\n",
    "#         + trainable_count\n",
    "#         + non_trainable_count\n",
    "#     )\n",
    "#     return total_memory\n",
    "\n",
    "# get_model_memory_usage(512, ncf_model)\n",
    "\n",
    "# keras_model_memory_usage_in_bytes(model=ncf_model, batch_size=512)/1000000000\n",
    "\n",
    "# from model_profiler import model_profiler\n",
    "\n",
    "# batch_size = 512\n",
    "# profile = model_profiler(ncf_model, batch_size)\n",
    "# print (profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb4a55",
   "metadata": {},
   "source": [
    "# Precision at K: Keras (Wrong Evaluation Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557092bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_n = 5\n",
    "# precision_scores = []\n",
    "# indices = list(range(data['test'].shape[0]))\n",
    "\n",
    "# import random\n",
    "# random.shuffle(indices)\n",
    "\n",
    "# # list(split(indices, 10))\n",
    "\n",
    "# for index_chunk in list(split(indices, split_n)):\n",
    "#     #print (index_chunk)\n",
    "#     print\n",
    "#     precision_ncf = tf.keras.metrics.Precision(top_k=TOP_K)\n",
    "#     recall_ncf = tf.keras.metrics.Recall(top_k=TOP_K)\n",
    "    \n",
    "# #     map_k = tf.compat.v1.metrics.average_precision_at_k(labels=data[\"test\"][index_chunk], predictions=data[\"ncf_predictions\"][index_chunk], k=TOP_K)\n",
    "#     #map_k.update_state(data[\"test\"][index_chunk], data[\"ncf_predictions\"][index_chunk])\n",
    "# #     print(\n",
    "# #         f\"At K = {TOP_K}, we have a MAP of {map_k.result().numpy():.5f}\"\n",
    "# #     )\n",
    "    \n",
    "# #     map_k = None\n",
    "\n",
    "#     precision_ncf.update_state(data[\"test\"][index_chunk], data[\"ncf_predictions\"][index_chunk])\n",
    "#     #recall_ncf.update_state(data[\"test\"][:80], data[\"ncf_predictions\"][:90])\n",
    "\n",
    "#     print(\n",
    "#         f\"At K = {TOP_K}, we have a precision of {precision_ncf.result().numpy():.5f}\"\n",
    "#     )\n",
    "#     precision_scores.append(precision_ncf.result().numpy())\n",
    "#     #, f\"and a recall of {recall_ncf.result().numpy():.5f}\"\n",
    "#     precision_ncf = None\n",
    "#     recall_ncf = None\n",
    "#     print ('')\n",
    "\n",
    "# print (np.mean(precision_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sort the list by second item of tuple\n",
    "def Sort_Tuple(tup): \n",
    "  \n",
    "    # reverse = None (Sorts in Ascending order) \n",
    "    # key is set to sort using second element of \n",
    "    # sublist lambda has been used \n",
    "    return(sorted(tup, key = lambda x: x[1], reverse=True))\n",
    "\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '32g')\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .config(\"spark.driver.memory\", \"64g\")\\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "sc = SparkContext.getOrCreate() \n",
    "sqlContext = SQLContext(sc)\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f75b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_list = list(df_ncf_predictions.index)\n",
    "\n",
    "df_results = pd.DataFrame(user_id_list, columns=['user_id'])\n",
    "\n",
    "file_name = 'user_zerobased_map_for_als_comparison.pkl'\n",
    "\n",
    "open_file = open(file_name, \"rb\")\n",
    "dict_user_id = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\n",
    "reverse_user_map = {v: k for k, v in dict_user_id.items()}\n",
    "\n",
    "df_results[\"MAP_15\"] = None\n",
    "df_results[\"NDCG_15\"] = None\n",
    "\n",
    "df_results[\"true_user_id\"] = df_results.user_id.map(reverse_user_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24aee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort_Tuple([x for x in list(enumerate(data['ncf_predictions'][9])) if not np.isnan(x[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in list(enumerate(data['ncf_predictions'][9])):\n",
    "#     print (x, x[1], np.isnan(x[1]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a0269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f358ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ncf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15144f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = list(range(data['test'].shape[0]))\n",
    "split_n = 4\n",
    "\n",
    "precision_scores_all = []\n",
    "recall_scores_all = []\n",
    "# ndcg_scores_all = []\n",
    "predictionAndLabelsAll = []\n",
    "\n",
    "for index_chunk in list(split(indices, split_n)):\n",
    "    predictionAndLabels = []\n",
    "\n",
    "    counter = 0\n",
    "    precision_scores = []\n",
    "    for i in index_chunk:\n",
    "        print (i)\n",
    "        if i % 10 == 0:\n",
    "            print (i)\n",
    "        \n",
    "#         m_ndcg = tf.keras.metrics.NDCGMetric(topn=15)\n",
    "#         m_ndcg.update_state(data[\"test\"][i], data[\"ncf_predictions\"][i])\n",
    "#         ndcg_scores_all.append(m_ndcg.result().numpy())\n",
    "#         m_ndcg.reset_states()\n",
    "        \n",
    "#         print (counter)\n",
    "#         counter += 1\n",
    "        ncf_predictions_tuple = list(enumerate(data['ncf_predictions'][i]))\n",
    "        ncf_predictions_tuple = [x for x in list(enumerate(data['ncf_predictions'][i])) if not np.isnan(x[1])]\n",
    "#         print (list(enumerate(data['ncf_predictions'][i])))\n",
    "        #test_tuple = list(enumerate(data['test'][i]))\n",
    "        #test_tuple = [x for x in list(enumerate(data['test'][i])) if not np.isnan(x[1])]\n",
    "        preds = [x[0] for x in Sort_Tuple(ncf_predictions_tuple)[0:100]]\n",
    "        #truth = [x[0] for x in Sort_Tuple(test_tuple) if x[1] == 1]\n",
    "        print (preds)\n",
    "        print (truth)\n",
    "        break\n",
    "#         predictionAndLabels.append((preds, truth))\n",
    "        predictionAndLabelsAll.append((preds, truth))\n",
    "#         predictionAndLabels_ind.append((preds, truth))\n",
    "#         metrics = RankingMetrics(predictionAndLabels_ind)\n",
    "#         print (metrics.meanAveragePrecisionAt(15))\n",
    "#     predictionAndLabels = sc.parallelize(predictionAndLabels)\n",
    "#     metrics = RankingMetrics(predictionAndLabels)\n",
    "#     print (metrics.precisionAt(15))\n",
    "#     print (metrics.meanAveragePrecisionAt(15))\n",
    "    break\n",
    "predictionAndLabels_All = sc.parallelize(predictionAndLabelsAll)\n",
    "metrics_all = RankingMetrics(predictionAndLabels_All)\n",
    "print (metrics_all.precisionAt(15))\n",
    "print (metrics_all.meanAveragePrecisionAt(15))\n",
    "print (metrics_all.ndcgAt(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b7762d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# precision_scores_all = []\n",
    "# recall_scores_all = []\n",
    "# # ndcg_scores_all = []\n",
    "# predictionAndLabelsAll = []\n",
    "\n",
    "# for index_chunk in list(split(indices, split_n)):\n",
    "#     predictionAndLabels = []\n",
    "\n",
    "#     counter = 0\n",
    "#     precision_scores = []\n",
    "#     for i in index_chunk:\n",
    "#         print (i)\n",
    "#         m_precision = tf.keras.metrics.Precision(top_k=15)\n",
    "#         m_precision.update_state(data[\"test\"][i], data[\"ncf_predictions\"][i])\n",
    "#         precision_scores.append(m_precision.result().numpy())\n",
    "#         precision_scores_all.append(m_precision.result().numpy())\n",
    "#         m_precision.reset_states()\n",
    "        \n",
    "#         m_recall = tf.keras.metrics.Recall(top_k=15)\n",
    "#         m_recall.update_state(data[\"test\"][i], data[\"ncf_predictions\"][i])\n",
    "#         recall_scores_all.append(m_recall.result().numpy())\n",
    "#         m_recall.reset_states()\n",
    "        \n",
    "# #         m_ndcg = tf.keras.metrics.NDCGMetric(topn=15)\n",
    "# #         m_ndcg.update_state(data[\"test\"][i], data[\"ncf_predictions\"][i])\n",
    "# #         ndcg_scores_all.append(m_ndcg.result().numpy())\n",
    "# #         m_ndcg.reset_states()\n",
    "        \n",
    "# #         print (counter)\n",
    "# #         counter += 1\n",
    "#         ncf_predictions_tuple = list(enumerate(data['ncf_predictions'][i]))\n",
    "#         test_tuple = list(enumerate(data['test'][i]))\n",
    "#         preds = [x[0] for x in Sort_Tuple(ncf_predictions_tuple)[0:100]]\n",
    "#         truth = [x[0] for x in Sort_Tuple(test_tuple) if x[1] == 1]\n",
    "#         predictionAndLabels.append((preds, truth))\n",
    "#         predictionAndLabelsAll.append((preds, truth))\n",
    "#     predictionAndLabels = sc.parallelize(predictionAndLabels)\n",
    "#     metrics = RankingMetrics(predictionAndLabels)\n",
    "#     print (metrics.precisionAt(15))\n",
    "#     print (metrics.meanAveragePrecisionAt(15))\n",
    "    \n",
    "# predictionAndLabels_All = sc.parallelize(predictionAndLabelsAll)\n",
    "# metrics_all = RankingMetrics(predictionAndLabels_All)\n",
    "# print (metrics_all.precisionAt(15))\n",
    "# print (metrics_all.meanAveragePrecisionAt(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5164392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predictionAndLabelsAll)):\n",
    "    prediction_and_labels = sc.parallelize(predictionAndLabelsAll[i:i+1])\n",
    "    metrics_all = RankingMetrics(prediction_and_labels)\n",
    "    df_results.loc[i, \"MAP_15\"] = metrics_all.meanAveragePrecisionAt(15)\n",
    "    df_results.loc[i, \"NDCG_15\"] = metrics_all.ndcgAt(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907de9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_results.to_pickle(\"df_truth_and_prediciton_NCF.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(precision_scores_all) == df_results.shape[0]\n",
    "# assert len(recall_scores_all) == df_results.shape[0]\n",
    "\n",
    "\n",
    "# df_results[\"MAP_15\"] = pd.Series(precision_scores_all)\n",
    "# df_results[\"Recall_15\"] = pd.Series(recall_scores_all)\n",
    "# df_results[\"NDCG_15\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17181b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results..user_id == 6974]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c68e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_history = pd.read_pickle('user_history.pkl')\n",
    "\n",
    "df_results = df_results.drop('user_id', axis=1)\n",
    "\n",
    "df_results = pd.merge(df_results, df_user_history, how='left', left_on='true_user_id', right_on='user_id')\n",
    "\n",
    "df_results.to_pickle(\"df_truth_and_prediciton_MF_Only_Dec28_v12.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5faa7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a65bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5bd262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff5922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = ncf_model.get_layer(\"mf_user_embedding\").get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b4d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['test'].shape\n",
    "\n",
    "ncf_model.get_layer(\"mf_user_embedding\").get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d837efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embeddings = ncf_model.get_layer(\"mf_item_embedding\").get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1cd240",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = ncf_model.get_layer(\"mf_user_embedding\").get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"product_embedding_MF.pkl\"\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(item_embeddings, open_file)\n",
    "open_file.close()\n",
    "\n",
    "file_name = \"user_embedding_MF.pkl\"\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(user_embeddings, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046ca5f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictionAndLabels = []\n",
    "# precision_scores = []\n",
    "# counter = 0\n",
    "# for i in range(0, 1315):\n",
    "#     print (counter)\n",
    "#     counter += 1\n",
    "#     ncf_predictions_tuple = list(enumerate(data['ncf_predictions'][i]))\n",
    "#     test_tuple = list(enumerate(data['test'][i]))\n",
    "#     preds = [x[0] for x in Sort_Tuple(ncf_predictions_tuple)[0:100]]\n",
    "#     truth = [x[0] for x in Sort_Tuple(test_tuple)[0:15] if x[1] == 1]\n",
    "#     predictionAndLabels.append((preds, truth))\n",
    "# predictionAndLabels = sc.parallelize(predictionAndLabels)\n",
    "# metrics = RankingMetrics(predictionAndLabels)\n",
    "# print (metrics.precisionAt(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictionAndLabels = sc.parallelize(predictionAndLabels)\n",
    "# metrics = RankingMetrics(predictionAndLabels)\n",
    "# print (metrics.precisionAt(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d31cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53806602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856987f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ff917",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf_predictions_tuple = list(enumerate(data['ncf_predictions'][1]))\n",
    "test_tuple = list(enumerate(data['test'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7febec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf_predictions_tuple[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['test'][1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sort_Tuple(test_tuple)[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e15d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x[0] for x in Sort_Tuple(test_tuple)[0:15] if x[1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bd1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x[0] for x in Sort_Tuple(ncf_predictions_tuple)[0:15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "hey.sort()\n",
    "list(reversed(hey))[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019fb0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_ncf.reset_states()\n",
    "\n",
    "precision_ncf = tf.keras.metrics.Precision(top_k=TOP_K)\n",
    "recall_ncf = tf.keras.metrics.Recall(top_k=TOP_K)\n",
    "\n",
    "precision_ncf.reset_states()\n",
    "precision_ncf.update_state(data[\"test\"][0:1315], data[\"ncf_predictions\"][0:1315])\n",
    "#recall_ncf.update_state(data[\"test\"][:80], data[\"ncf_predictions\"][:90])\n",
    "\n",
    "print(\n",
    "    f\"At K = {TOP_K}, we have a precision of {precision_ncf.result().numpy():.5f}\"\n",
    ")\n",
    "\n",
    "#, f\"and a recall of {recall_ncf.result().numpy():.5f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0395c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv(\"~/work/products.csv\", usecols=['product_id', 'product_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val = pd.merge(df_val, products, on='product_id', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['product_id_original']  = df_val.product_id.apply(lambda x: zerobased_index_to_product[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff7a2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.merge(df_val, products, left_on='product_id_original', right_on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe052af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.user_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b0b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ([zerobased_index_to_user[x] for x in df_val.user_id.unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc07c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a249597",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_study = 16274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val[(df_val.user_id == user_to_zerobased_index[user_to_study]) &\n",
    "       (df_val.interaction == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val[(df_val.user_id == user_to_zerobased_index[user_to_study])].sort_values(\"ncf_predictions\", ascending=False).iloc[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d9762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f55ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
