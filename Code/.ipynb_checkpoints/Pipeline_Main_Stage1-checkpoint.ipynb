{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main purpose of this .ipynb file is to train an ALS model to return the top X products\n",
    "# for each user in our population of interest. A pickle file containing all user_id & product_id\n",
    "# will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354c02ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/10 03:43:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import subprocess\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.mllib.recommendation import Rating\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from collections import defaultdict\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "#Create PySpark SparkSession\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '64g')\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .config(\"spark.driver.memory\", \"64g\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\n",
    "sc = SparkContext.getOrCreate() \n",
    "sqlContext = SQLContext(sc)\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a555a26",
   "metadata": {},
   "source": [
    "#### Saving and Loading 0-based User Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e33b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # convert user_id from a 1-based index to a 0-based index. dictionary is saved\n",
    "# # for future use.\n",
    "\n",
    "# orders = pd.read_csv(\"orders.csv\")\n",
    "# all_users = orders.user_id.unique()\n",
    "# all_users.sort()\n",
    "# user_study = np.random.choice(all_users, len(all_users)//2, replace=False)\n",
    "# user_study.sort()\n",
    "# print (len(user_study))\n",
    "# dict_user_study = dict(zip(user_study, range(len(user_study))))\n",
    "\n",
    "# file_name = \"user_study_dict_100k.pkl\"\n",
    "# open_file = open(file_name, \"wb\")\n",
    "# pickle.dump(dict_user_study, open_file)\n",
    "# open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "857010ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in 0-based user index map\n",
    "\n",
    "# file_name = 'user_study_dict_ALLUSERS.pkl'\n",
    "\n",
    "# open_file = open(file_name, \"rb\")\n",
    "# user_study_dict = pickle.load(open_file)\n",
    "# open_file.close()\n",
    "# study_indices = list(user_study_dict.keys())\n",
    "\n",
    "# file_name = 'user_study_dict_40k.pkl'\n",
    "\n",
    "# open_file = open(file_name, \"rb\")\n",
    "# user_study_dict = pickle.load(open_file)\n",
    "# open_file.close()\n",
    "# study_indices = list(user_study_dict.keys())\n",
    "\n",
    "# file_name = 'user_study_dict_100k.pkl'\n",
    "\n",
    "# open_file = open(file_name, \"rb\")\n",
    "# user_study_dict = pickle.load(open_file)\n",
    "# open_file.close()\n",
    "# study_indices = list(user_study_dict.keys())\n",
    "\n",
    "file_name = 'user_study_dict_20k.pkl'\n",
    "\n",
    "open_file = open(file_name, \"rb\")\n",
    "user_study_dict = pickle.load(open_file)\n",
    "open_file.close()\n",
    "study_indices = list(user_study_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba56ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pyspark dataframe to limit the sample of users we are considering for reccommendations\n",
    "\n",
    "study_indices = [str(x) for x in list(user_study_dict.keys())]\n",
    "R = Row('user_id')\n",
    "study_indices_df = spark.createDataFrame([R(x) for x in study_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c750b8",
   "metadata": {},
   "source": [
    "#### Loading Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7876b602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done reading data\n"
     ]
    }
   ],
   "source": [
    "# loading in all order history from user population\n",
    "\n",
    "# df_spark_orders_full = sqlContext.read.csv(\"orders.csv\", header=True).withColumn(\"order_number\", \n",
    "#                                                             col(\"order_number\").cast(\"int\"))\n",
    "\n",
    "df_spark_previous = sqlContext.read.csv(\"order_products__prior.csv\", header=True)\n",
    "df_spark_train = sqlContext.read.csv(\"order_products__train.csv\", header=True)\n",
    "df_spark_orders = sqlContext.read.csv(\"orders.csv\", header=True).select('order_id', 'user_id')\n",
    "\n",
    "\n",
    "df_spark_previous_joined = df_spark_previous.join(df_spark_orders, 'order_id', 'left')\n",
    "df_spark_previous_joined = df_spark_previous_joined.join(study_indices_df, how='inner', on='user_id')\n",
    "\n",
    "df_spark_train_joined = df_spark_train.join(df_spark_orders, 'order_id', 'left')\n",
    "df_spark_train_joined = df_spark_train_joined.join(study_indices_df, how='inner', on='user_id')\n",
    "\n",
    "print ('done reading data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5330370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping all product purchases by user_id. These dataframes will train and test \n",
    "# the ALS model.\n",
    "\n",
    "df_train_reorder_count = df_spark_train_joined.groupBy(\"user_id\", \"product_id\").agg(F.count('reordered').alias('reorder_count'))\n",
    "df_previous_reorder_count = df_spark_previous_joined.groupBy(\"user_id\", \"product_id\").agg(F.count('reordered').alias('reorder_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80eeb2fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating a StringIndexer for user_id and product_id. Converting all data to use new indexes. handleInvalid=keep\n",
    "# StringIndexer encodes a string column of labels to a column of label indices. If the input column is numeric, \n",
    "# we cast it to string and index the string values. The indices are in [0, numLabels). By default, this is ordered by \n",
    "# label frequencies so the most frequent label gets index 0. The ordering behavior is controlled by setting stringOrderType.\n",
    "\n",
    "# .setHandleInvalid(\"keep\") option adds new indexes when it sees new labels.\n",
    "\n",
    "indexer_user = StringIndexer(inputCol=\"user_id\", outputCol=\"user_index\", handleInvalid='keep').fit(df_previous_reorder_count)\n",
    "indexer_product = StringIndexer(inputCol=\"product_id\", outputCol=\"product_index\", handleInvalid='keep').fit(df_previous_reorder_count)\n",
    "\n",
    "df_previous_reorder_count_indexed = indexer_product.transform(indexer_user.transform(df_previous_reorder_count))\n",
    "df_train_reorder_count_indexed = indexer_product.transform(indexer_user.transform(df_train_reorder_count))\n",
    "\n",
    "# creating a map of the user_id and what they actually purchased in their last order\n",
    "true_product_list_train = df_train_reorder_count_indexed.select(\"user_index\", \"product_index\").groupby(\"user_index\").agg(F.expr('collect_list(product_index) AS true_priority'))\n",
    "\n",
    "# saving user_index for train, validation sets for later use\n",
    "train_users = df_train_reorder_count_indexed.select(\"user_index\").distinct()\n",
    "prior_users = df_previous_reorder_count_indexed.select(\"user_index\").distinct()\n",
    "\n",
    "# saving product_index for later use\n",
    "all_products = df_previous_reorder_count_indexed.select(\"product_index\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55c5208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving index map to convert StringIndexer index back to original index later. Creating 3 dataframes to store the maps for\n",
    "# train users, prior users and all products\n",
    "\n",
    "user_labels = indexer_user.labels\n",
    "product_labels = indexer_product.labels\n",
    "\n",
    "user_id_to_label = IndexToString(inputCol='user_index', outputCol='user_id', labels=user_labels)\n",
    "product_id_to_label = IndexToString(inputCol='product_index', outputCol='product_id', labels=product_labels)\n",
    "\n",
    "# train users\n",
    "\n",
    "train_users_with_labels = user_id_to_label.transform(train_users)\n",
    "df_train_users = train_users_with_labels.toPandas()\n",
    "\n",
    "# prior users\n",
    "\n",
    "prior_users_with_labels = user_id_to_label.transform(prior_users)\n",
    "df_prior_users = prior_users_with_labels.toPandas()\n",
    "\n",
    "# all products\n",
    "\n",
    "products_with_labels = product_id_to_label.transform(all_products)\n",
    "df_products_with_labels = products_with_labels.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d04d28",
   "metadata": {},
   "source": [
    "#### Run if Training and Model Save is Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fba85bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rank = 250\n",
    "alpha = 5.0\n",
    "regParam = 0.5\n",
    "\n",
    "maxIter = 2\n",
    "numUserBlocks = 10\n",
    "numItemBlocks = 10\n",
    "implicitPrefs = True\n",
    "nonnegative = False\n",
    "userCol = \"user_index\"\n",
    "itemCol = \"product_index\"\n",
    "ratingCol = \"reorder_count\"\n",
    "\n",
    "als = ALS(maxIter=maxIter,numUserBlocks=numUserBlocks, numItemBlocks=numItemBlocks,userCol=userCol, itemCol=itemCol, ratingCol=ratingCol, \n",
    "implicitPrefs=implicitPrefs, rank=rank, alpha=alpha, regParam=regParam)\n",
    "model = als.fit(df_previous_reorder_count_indexed)\n",
    "\n",
    "model.write().overwrite().save(\"ALS_model_rank250_20k.model\") #10k sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e794fa41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd33b0ea",
   "metadata": {},
   "source": [
    "#### Load Model \"ALS_model_rank250_count_comparison.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad48a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ALSModel.load(\"ALS_model_rank350_allusers.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f7aaa",
   "metadata": {},
   "source": [
    "#### Extracting User and Item Latent Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe74aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_itemFactors = (model.itemFactors).toPandas()\n",
    "# print (df_itemFactors.iloc[0]['features'].shape)\n",
    "# print (df_itemFactors.shape)\n",
    "\n",
    "# df_userFactors = model.userFactors.toPandas()\n",
    "# print (df_userFactors.iloc[0]['features'].shape)\n",
    "# print (df_userFactors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e04d23",
   "metadata": {},
   "source": [
    "#### Top 15 Recommendations per User (Validation Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "687487e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (MAP 15):  0.17986596815863914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 277:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (ndcg 15):  0.3045305556835871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recommendations_train = model.recommendForUserSubset(train_users, 15)\n",
    "predicted_priority_train = recommendations_train.select('user_index', 'recommendations.product_index')\n",
    "predicted_priority_train = predicted_priority_train.join(true_product_list_train, ['user_index'], \"left\")\n",
    "truth_and_prediction_train = predicted_priority_train.rdd.map(lambda row: (row[1], row[2]))\n",
    "rank_metrics_train = RankingMetrics(truth_and_prediction_train)\n",
    "\n",
    "print ('Train (MAP 15): ', rank_metrics_train.meanAveragePrecisionAt(15))\n",
    "# print ('Train (MAP 100): ', rank_metrics_train.meanAveragePrecisionAt(100))\n",
    "\n",
    "print ('Train (ndcg 15): ', rank_metrics_train.ndcgAt(15))\n",
    "# print ('Train (ndcg 100): ', rank_metrics_train.ndcgAt(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f8e084",
   "metadata": {},
   "source": [
    "#### Top (50, 100, 150) per User for all Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0ec58e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recommendations_train_cg = model.recommendForUserSubset(prior_users, 100)\n",
    "predicted_priority_train_cg = recommendations_train_cg.select('user_index', 'recommendations.product_index')\n",
    "\n",
    "predicted_priority_train_cg = predicted_priority_train_cg.join(true_product_list_train, ['user_index'], \"left\")\n",
    "truth_and_prediction_train_cg = predicted_priority_train_cg.rdd.map(lambda row: (row[1], row[2]))\n",
    "rank_metrics_train_cg = RankingMetrics(truth_and_prediction_train_cg)\n",
    "\n",
    "#### Bringing back Original Index for Each User\n",
    "\n",
    "predicted_priority_train_cg = user_id_to_label.transform(predicted_priority_train_cg)\n",
    "\n",
    "df_predicted_priority_train_cg = predicted_priority_train_cg.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e311ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_predicted_priority_train_cg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb86c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_predicted_priority_train_cg_exploded = df_predicted_priority_train_cg.explode('product_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419df978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_predicted_priority_train_cg_exploded[df_predicted_priority_train_cg_exploded.user_index == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad1de3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Exploding Top X Predictions into Row-Level Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d0845",
   "metadata": {},
   "source": [
    "#### Bringing back Original Index for Each Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5911209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2062000, 4) (20620, 4)\n"
     ]
    }
   ],
   "source": [
    "# expanding out the predictions, seen in the 'product_index' column, to become row-wise\n",
    "df_predicted_priority_train_cg_exploded = df_predicted_priority_train_cg.explode('product_index')\n",
    "\n",
    "print (df_predicted_priority_train_cg_exploded.shape, df_predicted_priority_train_cg.shape)\n",
    "\n",
    "# df_predicted_priority_train_cg_exploded.head()\n",
    "\n",
    "df_products_with_labels.product_index = df_products_with_labels.product_index.astype(int)\n",
    "\n",
    "# df_products_with_labels[df_products_with_labels.product_index == 8596]\n",
    "\n",
    "df_predicted_priority_train_cg_exploded = pd.merge(df_predicted_priority_train_cg_exploded, df_products_with_labels, how='left', on='product_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "128b0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_exploded = df_predicted_priority_train_cg_exploded[[\"user_id\", \"product_id\"]].copy()\n",
    "cg_exploded.user_id = cg_exploded.user_id.astype(np.int64) \n",
    "cg_exploded.product_id = cg_exploded.product_id.astype(np.int64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a492c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting user_id and product_id back to 0-based index\n",
    "cg_exploded.user_id = cg_exploded.user_id.map(user_study_dict)\n",
    "cg_exploded.product_id = cg_exploded.product_id - 1\n",
    "\n",
    "# format of this data is unique combinations of user_id, product_id\n",
    "cg_exploded.to_pickle('cg_exploded_rank250_20k_100.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f8d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdda1fef",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0acb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to time out rank 40 and rank 250 for all users\n",
    "\n",
    "possible_ranks = [40, 250]\n",
    "\n",
    "for rank_var in possible_ranks:\n",
    "    rank = rank_var\n",
    "    alpha = 5.0\n",
    "    regParam = 0.5\n",
    "    \n",
    "    maxIter = 2\n",
    "    numUserBlocks = 10\n",
    "    numItemBlocks = 10\n",
    "    implicitPrefs = True\n",
    "    nonnegative = False\n",
    "    userCol = \"user_index\"\n",
    "    itemCol = \"product_index\"\n",
    "    ratingCol = \"reorder_count\"\n",
    "    tic1 = time.perf_counter()\n",
    "    als = ALS(maxIter=maxIter,numUserBlocks=numUserBlocks, numItemBlocks=numItemBlocks,userCol=userCol, itemCol=itemCol, ratingCol=ratingCol, \n",
    "    implicitPrefs=implicitPrefs, rank=rank, alpha=alpha, regParam=regParam)\n",
    "    model = als.fit(df_previous_reorder_count_indexed)\n",
    "    toc1 = time.perf_counter()\n",
    "    tic2 = time.perf_counter()\n",
    "    recommendations_train = model.recommendForUserSubset(train_users, 15)\n",
    "    toc2 = time.perf_counter()\n",
    "    \n",
    "    predicted_priority_train = recommendations_train.select('user_index', 'recommendations.product_index')\n",
    "    predicted_priority_train = predicted_priority_train.join(true_product_list_train, ['user_index'], \"left\")\n",
    "    truth_and_prediction_train = predicted_priority_train.rdd.map(lambda row: (row[1], row[2]))\n",
    "    rank_metrics_train = RankingMetrics(truth_and_prediction_train)\n",
    "\n",
    "    print ('Train (MAP 15): ', rank_metrics_train.meanAveragePrecisionAt(15))\n",
    "    # print ('Train (MAP 100): ', rank_metrics_train.meanAveragePrecisionAt(100))\n",
    "\n",
    "    print ('Train (ndcg 15): ', rank_metrics_train.ndcgAt(15))\n",
    "    # print ('Train (ndcg 100): ', rank_metrics_train.ndcgAt(100))\n",
    "    \n",
    "    model.write().overwrite().save(\"ALS_model_rank\" + str(rank_var) + \"_allusers.model\") #10k sample\n",
    "    file1 = open(\"ALS_Rank_TimeTest.txt\",\"a\")\n",
    "    L = [\"rank: \" + str(rank_var) + \"\\n\",\n",
    "        \"Train Val MAP 15: \" + str(rank_metrics_train.meanAveragePrecisionAt(15)) + \"\\n\",\n",
    "        \"Train Val NDCG 15: \" + str(rank_metrics_train.ndcgAt(15)) + \"\\n\",\n",
    "        f\"Model trained in {toc1 - tic1:0.4f} seconds\", \n",
    "         f\"Predictions made in {toc2 - tic2:0.4f}\", \"\\n \\n\"]\n",
    "    file1.writelines(L)\n",
    "    file1.close() #to change file access modes\n",
    "#     model = None\n",
    "#     del model\n",
    "#     gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
